{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83c\udf0d SSE Renewables \u2014 BDH Climate Risk Pipeline  (Final Version)\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "NASA POWER CSV (hourly, 10 years)\n",
    "        \u2502\n",
    "        \u25bc\n",
    "[CELL 3] Feature Engineering \u2500\u2500\u25ba 84 features (wind, solar, meteo, temporal)\n",
    "        \u2502\n",
    "        \u25bc\n",
    "[CELL 4] BDH Model Training \u2500\u2500\u2500\u25ba Trained on 8 years of hourly data\n",
    "        \u2502                         Saves checkpoint: /content/bdh_checkpoint.pt\n",
    "        \u25bc\n",
    "[CELL 5] RAG Vector Index \u2500\u2500\u2500\u2500\u2500\u2500\u25ba SSE PDFs embedded \u2192 ChromaDB\n",
    "        \u2502\n",
    "        \u25bc\n",
    "[CELL 6] CORE PIPELINE LOOP\n",
    "  Every HOUR:\n",
    "    \u2514\u2500\u25ba BDH inference(window) \u2500\u2500\u25ba predicted + actual per feature\n",
    "    \u2514\u2500\u25ba Accumulate: wind stats, power proxy, memory_norm, per-feature MAE\n",
    "  Every MONTH (end of month):\n",
    "    \u2514\u2500\u25ba Build monthly_summary   (wind stats from BDH actuals)\n",
    "    \u2514\u2500\u25ba Build raw_predictions   (predicted vs actual, MAE, bias per feature)\n",
    "    \u2514\u2500\u25ba Build financials        (derived from BDH wind output)\n",
    "    \u2514\u2500\u25ba Package bdh_data_for_llm\n",
    "    \u2514\u2500\u25ba ask_analyst(bdh_data=bdh_data_for_llm)\n",
    "          \u251c\u2500\u25ba RAG retrieval (SSE docs) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "          \u251c\u2500\u25ba BDH context built from bdh_data (real values, no N/A) \u2502\n",
    "          \u2514\u2500\u25ba Groq LLM (llama-3.3-70b) \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                \u2514\u2500\u25ba Structured 6-section monthly report\n",
    "    \u2514\u2500\u25ba JSON record saved (key_numbers + llm_conclusion sections)\n",
    "        \u2502\n",
    "        \u25bc\n",
    "[CELL 7] REST API (optional)\n",
    "  POST /ask              \u2192 query LLM analyst with any question\n",
    "  GET  /monthly-reports  \u2192 download full 2-year JSON\n",
    "  GET  /live-state       \u2192 current BDH state\n",
    "        \u2502\n",
    "        \u25bc\n",
    "[CELL 8] Interactive Analyst (CLI menu in Colab)\n",
    "```\n",
    "\n",
    "## Execution Order  \u2190 DO NOT SKIP ANY CELL\n",
    "```\n",
    "Cell 1 \u2192 Install dependencies\n",
    "Cell 2 \u2192 Global config & API key\n",
    "Cell 3 \u2192 Feature engineering (NASA CSV \u2192 84 features)\n",
    "Cell 4 \u2192 Train BDH model (8 years, ~10 epochs)\n",
    "Cell 5 \u2192 Upload SSE PDFs \u2192 build RAG index\n",
    "Cell 6 \u2192 Run full 2-year pipeline \u2192 monthly JSON output  \u2190 CORE\n",
    "Cell 7 \u2192 (Optional) Launch REST API\n",
    "Cell 8 \u2192 (Optional) Interactive CLI analyst\n",
    "```\n",
    "\n",
    "## Key Fixes in This Version\n",
    "- \u2705 BDH output (all 84 features) accumulated per-hour and passed directly into LLM\n",
    "- \u2705 Per-feature predicted vs actual means, MAE, bias sent to LLM context\n",
    "- \u2705 LLM grounding instruction: every number must trace back to BDH output\n",
    "- \u2705 JSON output: key_numbers + LLM narrative split into 6 structured sections\n",
    "- \u2705 ask_analyst() accepts bdh_data= directly (no more N/A from LIVE_STATE mismatch)\n"
   ],
   "metadata": {
    "id": "markdown_intro"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  CELL 1 \u2014 Install All Dependencies                          \u2551\n",
    "# \u2551  Run this first. Runtime restart may be required after.     \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "!pip install -q pathway\n",
    "!pip install -q groq\n",
    "!pip install -q -U langchain langchain-community langchain-text-splitters langchain-huggingface\n",
    "!pip install -q chromadb\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q pypdf\n",
    "!pip install -q windpowerlib pvlib\n",
    "\n",
    "print(\"\u2705 All dependencies installed. Proceed to Cell 2.\")"
   ],
   "metadata": {
    "id": "cell1_install"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  CELL 2 \u2014 Global Config & Constants                         \u2551\n",
    "# \u2551  All settings in one place. Edit here, not in later cells.  \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "import torch\n",
    "\n",
    "# \u2500\u2500 API Key (secure \u2014 never hardcode the actual key) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your GROQ_API_KEY: \")\n",
    "\n",
    "# \u2500\u2500 LLM settings \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "LLM_MODEL  = \"llama-3.3-70b-versatile\"\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "# \u2500\u2500 RAG settings \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "EMBEDDING_MODEL  = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE       = 1000\n",
    "CHUNK_OVERLAP    = 150\n",
    "RETRIEVAL_TOP_K  = 5\n",
    "REPORTS_DIR      = \"/content/reports\"\n",
    "CHROMA_DIR       = \"/content/chroma_db\"\n",
    "\n",
    "# \u2500\u2500 BDH / Training settings \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "FEATURE_DIM = 84      # number of input features after engineering\n",
    "SEQ_LEN     = 32      # sliding-window length (hours)\n",
    "BATCH_SIZE  = 64      # A100-safe\n",
    "EPOCHS      = 10\n",
    "LR          = 3e-4\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# \u2500\u2500 LLM call frequency: once per month (after all hourly BDH rows processed) \u2500\u2500\n",
    "# No number needed \u2014 LLM fires at the END of each monthly batch automatically\n",
    "\n",
    "# \u2500\u2500 Fallback demo docs if no PDFs are uploaded \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "from langchain_core.documents import Document\n",
    "DEMO_DOCS = [\n",
    "    Document(\n",
    "        page_content=\"SSE targets net zero by 2050 with an 80% emissions reduction by 2030. \"\n",
    "                     \"The company has committed to investing \u00a318bn in low-carbon infrastructure \"\n",
    "                     \"over the next 5 years, focused on wind, solar and electricity networks.\",\n",
    "        metadata={\"source\": \"demo\", \"source_file\": \"demo_sse_strategy.txt\",\n",
    "                  \"report_year\": \"2023\", \"page\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Physical climate risks for SSE include increased storm frequency, \"\n",
    "                     \"wind variability across UK and Irish assets, and rising sea levels \"\n",
    "                     \"affecting coastal infrastructure. These are classified as high-likelihood \"\n",
    "                     \"medium-impact risks under the TCFD framework.\",\n",
    "        metadata={\"source\": \"demo\", \"source_file\": \"demo_sse_tcfd.txt\",\n",
    "                  \"report_year\": \"2023\", \"page\": 2}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"SSE ESG metrics include carbon intensity (tCO2e/GWh), renewable capacity \"\n",
    "                     \"(GW), total energy generated (TWh), and percentage of capital expenditure \"\n",
    "                     \"aligned to EU Taxonomy. Current renewable capacity stands at 4.7 GW.\",\n",
    "        metadata={\"source\": \"demo\", \"source_file\": \"demo_sse_esg.txt\",\n",
    "                  \"report_year\": \"2023\", \"page\": 3}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Transition risks for SSE include carbon pricing mechanisms, policy changes \"\n",
    "                     \"to Contracts for Difference (CfD) strike prices, and changing grid \"\n",
    "                     \"balancing requirements as renewable penetration increases in GB and Ireland.\",\n",
    "        metadata={\"source\": \"demo\", \"source_file\": \"demo_sse_transition.txt\",\n",
    "                  \"report_year\": \"2023\", \"page\": 4}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"\u2705 Config loaded\")\n",
    "print(f\"   Device   : {DEVICE}\")\n",
    "print(f\"   LLM Model: {LLM_MODEL}\")\n",
    "print(f\"   Embeddings: {EMBEDDING_MODEL}\")"
   ],
   "metadata": {
    "id": "cell2_config"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  CELL 3 \u2014 Feature Engineering (NASA Data \u2192 84 Features)     \u2551\n",
    "# \u2551  Loads NASA POWER CSV, cleans it, builds all feature groups  \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# \u2500\u2500 Load NASA POWER data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Replace path below with your actual file\n",
    "df = pd.read_csv(\"/content/nasa_power_2015to2025_hourly_data_ENGLAND.csv\")\n",
    "print(\"Raw data shape:\", df.shape)\n",
    "print(df.head(3))\n",
    "\n",
    "# Build datetime from YEAR, MO, DY, HR columns\n",
    "df[\"datetime\"] = pd.to_datetime(\n",
    "    df[[\"YEAR\",\"MO\",\"DY\",\"HR\"]].rename(\n",
    "        columns={\"MO\":\"month\",\"DY\":\"day\",\"HR\":\"hour\",\"YEAR\":\"year\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add spatial columns (replace with your actual site coordinates)\n",
    "df[\"lat\"]       = 51.5    # England site latitude\n",
    "df[\"lon\"]       = -1.8    # England site longitude\n",
    "df[\"elevation\"] = 75.0    # metres\n",
    "\n",
    "df.drop(columns=[\"YEAR\",\"MO\",\"DY\",\"HR\"], inplace=True)\n",
    "\n",
    "# \u2500\u2500 1. Data Cleaning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def clean_data(df):\n",
    "    df = df.copy()\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "    df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
    "    num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    missing = df[num_cols].isnull().sum()\n",
    "    if missing.any():\n",
    "        print(\"Missing values found \u2014 interpolating:\\n\", missing[missing > 0])\n",
    "    df[num_cols] = df[num_cols].interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "    for col in [\"T2M\", \"RH2M\", \"PS\", \"WS10M\", \"WS50M\", \"ALLSKY_SFC_SW_DWN\"]:\n",
    "        if col in df.columns:\n",
    "            q1, q3 = df[col].quantile(0.01), df[col].quantile(0.99)\n",
    "            iqr = q3 - q1\n",
    "            df[col] = df[col].clip(q1 - 1.5*iqr, q3 + 1.5*iqr)\n",
    "    df[\"RH2M\"]              = df[\"RH2M\"].clip(0, 100)\n",
    "    df[\"CLOUD_AMT\"]         = df[\"CLOUD_AMT\"].clip(0, 100)\n",
    "    df[\"ALLSKY_SFC_SW_DWN\"] = df[\"ALLSKY_SFC_SW_DWN\"].clip(0)\n",
    "    df[\"WS10M\"]             = df[\"WS10M\"].clip(0)\n",
    "    df[\"WS50M\"]             = df[\"WS50M\"].clip(0)\n",
    "    df[\"WD10M\"]             = df[\"WD10M\"].clip(0, 360)\n",
    "    df[\"WD50M\"]             = df[\"WD50M\"].clip(0, 360)\n",
    "    return df\n",
    "\n",
    "df = clean_data(df)\n",
    "print(\"After cleaning:\", df.shape)\n",
    "\n",
    "# \u2500\u2500 2. Temporal Features \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def add_temporal_features(df):\n",
    "    df = df.copy()\n",
    "    dt = df[\"datetime\"]\n",
    "    df[\"hour\"]        = dt.dt.hour\n",
    "    df[\"day_of_year\"] = dt.dt.dayofyear\n",
    "    df[\"month\"]       = dt.dt.month\n",
    "    df[\"weekday\"]     = dt.dt.weekday\n",
    "    df[\"is_weekend\"]  = (df[\"weekday\"] >= 5).astype(int)\n",
    "    df[\"hour_sin\"]    = np.sin(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"hour_cos\"]    = np.cos(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"doy_sin\"]     = np.sin(2*np.pi*df[\"day_of_year\"]/365)\n",
    "    df[\"doy_cos\"]     = np.cos(2*np.pi*df[\"day_of_year\"]/365)\n",
    "    df[\"month_sin\"]   = np.sin(2*np.pi*df[\"month\"]/12)\n",
    "    df[\"month_cos\"]   = np.cos(2*np.pi*df[\"month\"]/12)\n",
    "    def get_season(m):\n",
    "        if m in [12,1,2]:  return \"winter\"\n",
    "        if m in [3,4,5]:   return \"spring\"\n",
    "        if m in [6,7,8]:   return \"summer\"\n",
    "        return \"autumn\"\n",
    "    seasons = df[\"month\"].map(get_season)\n",
    "    df = pd.get_dummies(df, columns=[\"month\"], prefix=\"month\", drop_first=False)\n",
    "    for s in [\"winter\",\"spring\",\"summer\",\"autumn\"]:\n",
    "        df[f\"season_{s}\"] = (seasons == s).astype(int)\n",
    "    for lag in [1,3,6,24]:\n",
    "        df[f\"T2M_lag{lag}h\"]               = df[\"T2M\"].shift(lag)\n",
    "        df[f\"WS10M_lag{lag}h\"]             = df[\"WS10M\"].shift(lag)\n",
    "        df[f\"ALLSKY_SFC_SW_DWN_lag{lag}h\"] = df[\"ALLSKY_SFC_SW_DWN\"].shift(lag)\n",
    "    for win in [3,6,24]:\n",
    "        df[f\"T2M_roll_mean{win}h\"]               = df[\"T2M\"].rolling(win, min_periods=1).mean()\n",
    "        df[f\"T2M_roll_std{win}h\"]                = df[\"T2M\"].rolling(win, min_periods=1).std().fillna(0)\n",
    "        df[f\"WS10M_roll_mean{win}h\"]             = df[\"WS10M\"].rolling(win, min_periods=1).mean()\n",
    "        df[f\"WS10M_roll_std{win}h\"]              = df[\"WS10M\"].rolling(win, min_periods=1).std().fillna(0)\n",
    "        df[f\"ALLSKY_SFC_SW_DWN_roll_mean{win}h\"] = df[\"ALLSKY_SFC_SW_DWN\"].rolling(win, min_periods=1).mean()\n",
    "    return df\n",
    "\n",
    "df = add_temporal_features(df)\n",
    "print(\"After temporal features:\", df.shape)\n",
    "\n",
    "# \u2500\u2500 3. Spatial Features \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def add_spatial_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"lat_norm\"] = (df[\"lat\"] - df[\"lat\"].mean()) / (df[\"lat\"].std() + 1e-9)\n",
    "    df[\"lon_norm\"] = (df[\"lon\"] - df[\"lon\"].mean()) / (df[\"lon\"].std() + 1e-9)\n",
    "    lat_rad = np.radians(df[\"lat\"])\n",
    "    dec_rad = np.radians(23.45 * np.sin(2*np.pi*(df[\"day_of_year\"]-81)/365))\n",
    "    df[\"solar_noon_angle\"] = np.degrees(\n",
    "        np.arcsin(np.sin(lat_rad)*np.sin(dec_rad) + np.cos(lat_rad)*np.cos(dec_rad))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = add_spatial_features(df)\n",
    "print(\"After spatial features:\", df.shape)\n",
    "\n",
    "# \u2500\u2500 4. Solar Features \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def add_solar_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = df[\"datetime\"].dt.date\n",
    "    daily_irr = df.groupby(\"date\")[\"ALLSKY_SFC_SW_DWN\"].sum().rename(\"daily_irr_sum_Wh\")\n",
    "    df = df.merge(daily_irr, on=\"date\", how=\"left\")\n",
    "    df[\"ym\"] = df[\"datetime\"].dt.to_period(\"M\")\n",
    "    monthly_irr = df.groupby(\"ym\")[\"ALLSKY_SFC_SW_DWN\"].mean().rename(\"monthly_irr_mean\")\n",
    "    df = df.merge(monthly_irr, on=\"ym\", how=\"left\")\n",
    "    df.drop(columns=[\"date\",\"ym\"], inplace=True)\n",
    "    CLEAR_SKY_MAX = 1000.0\n",
    "    df[\"clearness_index\"]  = (df[\"ALLSKY_SFC_SW_DWN\"] / CLEAR_SKY_MAX).clip(0,1)\n",
    "    lat_rad = np.radians(df[\"lat\"])\n",
    "    dec_rad = np.radians(23.45 * np.sin(2*np.pi*(df[\"day_of_year\"]-81)/365))\n",
    "    cos_ha  = (-np.tan(lat_rad) * np.tan(dec_rad)).clip(-1,1)\n",
    "    df[\"daylight_hours\"]   = (2/15) * np.degrees(np.arccos(cos_ha))\n",
    "    df[\"daylight_fraction\"]= df[\"daylight_hours\"] / 24\n",
    "    df[\"T2M_sq\"]           = df[\"T2M\"]**2\n",
    "    df[\"T2M_x_cloud\"]      = df[\"T2M\"] * df[\"CLOUD_AMT\"]\n",
    "    df[\"irr_x_clearness\"]  = df[\"ALLSKY_SFC_SW_DWN\"] * df[\"clearness_index\"]\n",
    "    tilt_rad = lat_rad\n",
    "    df[\"tilt_adjusted_irr\"] = df[\"ALLSKY_SFC_SW_DWN\"] * (\n",
    "        np.sin(np.radians(df[\"solar_noon_angle\"]) + tilt_rad) /\n",
    "        np.maximum(np.sin(np.radians(df[\"solar_noon_angle\"])), 0.01)\n",
    "    ).clip(0, 2)\n",
    "    return df\n",
    "\n",
    "df = add_solar_features(df)\n",
    "print(\"After solar features:\", df.shape)\n",
    "\n",
    "# \u2500\u2500 5. Wind Features \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def add_wind_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"wind_power_10m\"]        = 0.5 * df[\"RHOA\"] * df[\"WS10M\"]**3\n",
    "    df[\"wind_power_50m\"]        = 0.5 * df[\"RHOA\"] * df[\"WS50M\"]**3\n",
    "    df[\"wind_shear_ratio\"]      = df[\"WS50M\"] / (df[\"WS10M\"] + 1e-9)\n",
    "    df[\"wind_shear_diff\"]       = df[\"WS50M\"] - df[\"WS10M\"]\n",
    "    df[\"WD10M_sin\"]             = np.sin(np.radians(df[\"WD10M\"]))\n",
    "    df[\"WD10M_cos\"]             = np.cos(np.radians(df[\"WD10M\"]))\n",
    "    df[\"WD50M_sin\"]             = np.sin(np.radians(df[\"WD50M\"]))\n",
    "    df[\"WD50M_cos\"]             = np.cos(np.radians(df[\"WD50M\"]))\n",
    "    df[\"WS10M_std_24h\"]         = df[\"WS10M\"].rolling(24, min_periods=1).std().fillna(0)\n",
    "    df[\"WS10M_above6_frac24\"]   = (df[\"WS10M\"] > 6).rolling(24, min_periods=1).mean()\n",
    "    T  = df[\"T2M\"]\n",
    "    RH = df[\"RH2M\"].clip(1,100)\n",
    "    df[\"dew_point\"]             = T - ((100 - RH) / 5.0)\n",
    "    df[\"air_density_T_corrected\"] = df[\"RHOA\"] * (273.15 / (273.15 + df[\"T2M\"]))\n",
    "    return df\n",
    "\n",
    "df = add_wind_features(df)\n",
    "print(\"After wind features:\", df.shape)\n",
    "\n",
    "# \u2500\u2500 6. Meteo Features \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def add_meteo_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"PS_trend_1h\"]      = df[\"PS\"].diff(1).fillna(0)\n",
    "    df[\"PS_trend_3h\"]      = df[\"PS\"].diff(3).fillna(0)\n",
    "    df[\"QV10M_roll_mean6h\"] = df[\"QV10M\"].rolling(6, min_periods=1).mean()\n",
    "    return df\n",
    "\n",
    "df = add_meteo_features(df)\n",
    "print(\"After meteo features:\", df.shape)\n",
    "\n",
    "# \u2500\u2500 7. Scaling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# NOTE: month_sin / month_cos are the actual column names (not month_x)\n",
    "EXCLUDE = [\"lat\",\"lon\",\"elevation\",\"is_weekend\",\n",
    "           \"hour\",\"weekday\",\"day_of_year\",\"month_sin\",\"month_cos\"]\n",
    "\n",
    "def scale_features(df, exclude_cols=None):\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    dt_cols  = df.select_dtypes(include=[\"datetime64\",\"object\",\"bool\"]).columns.tolist()\n",
    "    skip     = list(set(dt_cols + exclude_cols))\n",
    "    num_cols = [c for c in df.select_dtypes(include=np.number).columns if c not in skip]\n",
    "    scaler   = StandardScaler()\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "    return df_scaled, scaler, num_cols\n",
    "\n",
    "df_scaled, scaler, scaled_cols = scale_features(df, exclude_cols=EXCLUDE)\n",
    "print(f\"Scaled {len(scaled_cols)} numeric columns.\")\n",
    "\n",
    "# \u2500\u2500 8. Prepare train/test splits for BDH \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Drop non-numeric columns (datetime, object) before passing to BDH\n",
    "feature_cols = df_scaled.select_dtypes(include=np.number).columns.tolist()\n",
    "df_numeric   = df_scaled[feature_cols].fillna(0)\n",
    "\n",
    "# Trim to exact FEATURE_DIM columns if needed\n",
    "if len(feature_cols) > FEATURE_DIM:\n",
    "    feature_cols = feature_cols[:FEATURE_DIM]\n",
    "    df_numeric   = df_numeric[feature_cols]\n",
    "    print(f\"\u26a0\ufe0f  Trimmed to first {FEATURE_DIM} features\")\n",
    "elif len(feature_cols) < FEATURE_DIM:\n",
    "    # Pad with zeros if we have fewer features than expected\n",
    "    pad_cols = [f\"pad_{i}\" for i in range(FEATURE_DIM - len(feature_cols))]\n",
    "    for c in pad_cols:\n",
    "        df_numeric[c] = 0.0\n",
    "    feature_cols = list(df_numeric.columns)\n",
    "    print(f\"\u26a0\ufe0f  Padded to {FEATURE_DIM} features with zeros\")\n",
    "\n",
    "# 8-year train / 2-year test split\n",
    "split_idx  = int(len(df_numeric) * 0.8)\n",
    "df_train   = df_numeric.iloc[:split_idx].reset_index(drop=True)\n",
    "df_test    = df_numeric.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "# Keep datetime index for the test set (used during streaming)\n",
    "test_datetimes = df[\"datetime\"].iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n\u2705 Feature engineering complete\")\n",
    "print(f\"   Total features : {len(feature_cols)}\")\n",
    "print(f\"   Train rows     : {len(df_train):,}\")\n",
    "print(f\"   Test rows      : {len(df_test):,}\")\n",
    "print(f\"   Feature names  : {feature_cols[:5]} ... (first 5)\")"
   ],
   "metadata": {
    "id": "cell3_features"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  CELL 4 \u2014 BDH Model Definition + Training on 8 Years        \u2551\n",
    "# \u2551  MUST complete before Cell 5 or Cell 6.                     \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "import math\n",
    "import dataclasses\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# \u2500\u2500 BDH Config \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "@dataclasses.dataclass\n",
    "class BDHConfig:\n",
    "    n_layer: int = 4\n",
    "    n_embd:  int = 128\n",
    "    dropout: float = 0.1\n",
    "    n_head:  int = 4\n",
    "    mlp_internal_dim_multiplier: int = 16\n",
    "    vocab_size: int = 256\n",
    "\n",
    "# \u2500\u2500 Attention (RoPE-based) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def get_freqs(n, theta, dtype):\n",
    "    def quantize(t, q=2):\n",
    "        return (t / q).floor() * q\n",
    "    return (\n",
    "        1.0 / (theta ** (quantize(torch.arange(0, n, 1, dtype=dtype)) / n))\n",
    "        / (2 * math.pi)\n",
    "    )\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        nh = config.n_head\n",
    "        D  = config.n_embd\n",
    "        N  = config.mlp_internal_dim_multiplier * D // nh\n",
    "        self.freqs = torch.nn.Buffer(\n",
    "            get_freqs(N, theta=2**16, dtype=torch.float32).view(1,1,1,N)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def phases_cos_sin(phases):\n",
    "        phases = (phases % 1) * (2 * math.pi)\n",
    "        return torch.cos(phases), torch.sin(phases)\n",
    "\n",
    "    @staticmethod\n",
    "    def rope(phases, v):\n",
    "        v_rot = torch.stack((-v[...,1::2], v[...,::2]), dim=-1).view(*v.size())\n",
    "        pc, ps = Attention.phases_cos_sin(phases)\n",
    "        return (v*pc).to(v.dtype) + (v_rot*ps).to(v.dtype)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        assert self.freqs.dtype == torch.float32\n",
    "        assert K is Q\n",
    "        _, _, T, _ = Q.size()\n",
    "        r_phases = (\n",
    "            torch.arange(0, T, device=self.freqs.device, dtype=self.freqs.dtype)\n",
    "            .view(1,1,-1,1)\n",
    "        ) * self.freqs\n",
    "        QR     = self.rope(r_phases, Q)\n",
    "        scores = (QR @ QR.mT).tril(diagonal=-1)\n",
    "        return scores @ V\n",
    "\n",
    "# \u2500\u2500 BDH Model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "class BDH(nn.Module):\n",
    "    def __init__(self, config, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        nh = config.n_head\n",
    "        D  = config.n_embd\n",
    "        N  = D * config.mlp_internal_dim_multiplier // nh\n",
    "        self.input_proj = nn.Linear(input_dim, D)\n",
    "        self.decoder    = nn.Parameter(torch.zeros(nh*N, D).normal_(std=0.02))\n",
    "        self.encoder    = nn.Parameter(torch.zeros(nh, D, N).normal_(std=0.02))\n",
    "        self.encoder_v  = nn.Parameter(torch.zeros(nh, D, N).normal_(std=0.02))\n",
    "        self.attn = Attention(config)\n",
    "        self.ln   = nn.LayerNorm(D, elementwise_affine=False, bias=False)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.head = nn.Linear(D, output_dim)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T, _ = x.size()\n",
    "        D  = self.config.n_embd\n",
    "        nh = self.config.n_head\n",
    "        N  = D * self.config.mlp_internal_dim_multiplier // nh\n",
    "        x = self.input_proj(x)\n",
    "        x = self.ln(x).unsqueeze(1)\n",
    "        for _ in range(self.config.n_layer):\n",
    "            x_res    = x\n",
    "            x_latent = x @ self.encoder\n",
    "            x_sparse = F.relu(x_latent)\n",
    "            yKV      = self.attn(Q=x_sparse, K=x_sparse, V=x)\n",
    "            yKV      = self.ln(yKV)\n",
    "            y_latent  = yKV @ self.encoder_v\n",
    "            y_sparse  = F.relu(y_latent)\n",
    "            xy_sparse = self.drop(x_sparse * y_sparse)\n",
    "            yMLP = (\n",
    "                xy_sparse.transpose(1,2).reshape(B,1,T,N*nh) @ self.decoder\n",
    "            )\n",
    "            x = self.ln(x_res + self.ln(yMLP))\n",
    "        out    = x.view(B, T, D)\n",
    "        logits = self.head(out)\n",
    "        loss   = None\n",
    "        if targets is not None:\n",
    "            loss = F.mse_loss(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "# \u2500\u2500 Dataset \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data    = torch.tensor(data, dtype=torch.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.data) - self.seq_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.seq_len]\n",
    "        y = self.data[idx + 1 : idx + self.seq_len + 1]\n",
    "        return x, y\n",
    "\n",
    "# \u2500\u2500 Training function \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def train_bdh(model, df_train, epochs=EPOCHS):\n",
    "    data    = df_train.values.astype(np.float32)\n",
    "    dataset = TimeSeriesDataset(data, SEQ_LEN)\n",
    "    loader  = DataLoader(\n",
    "        dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=2, pin_memory=True,\n",
    "    )\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            _, loss = model(x, targets=y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg = total_loss / len(loader)\n",
    "        print(f\"  Epoch {epoch+1:02d}/{epochs} | Loss: {avg:.6f}\")\n",
    "    print(\"\u2705 Training complete.\")\n",
    "    return model\n",
    "\n",
    "# \u2500\u2500 Inference function \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "@torch.no_grad()\n",
    "def run_inference(model, window):\n",
    "    \"\"\"\n",
    "    window : (SEQ_LEN, FEATURE_DIM) numpy array\n",
    "    Returns predictions (SEQ_LEN, output_dim) and memory_norm (float)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    B  = 1\n",
    "    x  = torch.tensor(window, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "    T  = x.size(1)\n",
    "    D  = model.config.n_embd\n",
    "    nh = model.config.n_head\n",
    "    N  = D * model.config.mlp_internal_dim_multiplier // nh\n",
    "    h  = model.input_proj(x)\n",
    "    h  = model.ln(h).unsqueeze(1)\n",
    "    for _ in range(model.config.n_layer):\n",
    "        h_res    = h\n",
    "        x_latent = h @ model.encoder\n",
    "        x_sparse = F.relu(x_latent)\n",
    "        yKV      = model.attn(Q=x_sparse, K=x_sparse, V=h)\n",
    "        yKV      = model.ln(yKV)\n",
    "        y_sparse = F.relu(yKV @ model.encoder_v)\n",
    "        xy       = model.drop(x_sparse * y_sparse)\n",
    "        yMLP     = xy.transpose(1,2).reshape(B,1,T,N*nh) @ model.decoder\n",
    "        h        = model.ln(h_res + model.ln(yMLP))\n",
    "    out         = h.view(B, T, D)\n",
    "    logits      = model.head(out)\n",
    "    memory_norm = float(out[0,-1].norm().item())\n",
    "    return logits.squeeze(0).cpu().numpy(), memory_norm\n",
    "\n",
    "# \u2500\u2500 Build & train the model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(f\"Building BDH model on {DEVICE}...\")\n",
    "bdh_config = BDHConfig()\n",
    "bdh_model  = BDH(bdh_config, input_dim=FEATURE_DIM, output_dim=FEATURE_DIM).to(DEVICE)\n",
    "print(f\"BDH parameters: {sum(p.numel() for p in bdh_model.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nTraining BDH on {len(df_train):,} rows ({EPOCHS} epochs)...\")\n",
    "bdh_model = train_bdh(bdh_model, df_train, epochs=EPOCHS)\n",
    "\n",
    "# Save checkpoint\n",
    "torch.save(bdh_model.state_dict(), \"/content/bdh_checkpoint.pt\")\n",
    "print(\"\\n\u2705 Checkpoint saved to /content/bdh_checkpoint.pt\")\n",
    "print(\"   Proceed to Cell 5 to build the RAG index.\")"
   ],
   "metadata": {
    "id": "cell4_bdh_train"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  CELL 5 \u2014 Upload SSE PDFs & Build RAG Vector Index          \u2551\n",
    "# \u2551  MUST run after Cell 4 and before Cell 6.                   \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from google.colab import files\n",
    "\n",
    "# \u2500\u2500 Step 1: Upload PDFs \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\ud83d\udcc2 Upload your SSE PDF reports now...\")\n",
    "print(\"   (annual reports, TCFD reports, ESG disclosures, investor presentations)\")\n",
    "print(\"   Press Cancel/Skip if you want to use built-in demo documents instead.\\n\")\n",
    "\n",
    "try:\n",
    "    uploaded = files.upload()\n",
    "    for filename in uploaded.keys():\n",
    "        src = f\"/content/{filename}\"\n",
    "        dst = f\"{REPORTS_DIR}/{filename}\"\n",
    "        if os.path.exists(src):\n",
    "            shutil.move(src, dst)\n",
    "    report_files = os.listdir(REPORTS_DIR)\n",
    "    print(f\"\\n\u2705 {len(report_files)} file(s) in reports directory:\")\n",
    "    for f in report_files:\n",
    "        size_mb = os.path.getsize(f\"{REPORTS_DIR}/{f}\") / 1024 / 1024\n",
    "        print(f\"   \ud83d\udcc4 {f}  ({size_mb:.1f} MB)\")\n",
    "except Exception:\n",
    "    print(\"\u26a0\ufe0f  Upload skipped \u2014 will use demo documents.\")\n",
    "\n",
    "# \u2500\u2500 Step 2: Load documents \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def load_documents(reports_dir):\n",
    "    docs      = []\n",
    "    pdf_files = list(Path(reports_dir).glob(\"**/*.pdf\"))\n",
    "    csv_files = list(Path(reports_dir).glob(\"**/*.csv\"))\n",
    "    all_files = pdf_files + csv_files\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"\u26a0\ufe0f  No files found \u2014 loading DEMO documents.\")\n",
    "        return DEMO_DOCS\n",
    "\n",
    "    print(f\"\ud83d\udcc2 Found {len(all_files)} file(s) ({len(pdf_files)} PDFs, {len(csv_files)} CSVs)\")\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            if file_path.suffix.lower() == \".pdf\":\n",
    "                loader = PyPDFLoader(str(file_path))\n",
    "            else:\n",
    "                loader = TextLoader(str(file_path), encoding=\"utf-8\")\n",
    "            file_docs = loader.load()\n",
    "            for doc in file_docs:\n",
    "                doc.metadata[\"source_file\"] = file_path.name\n",
    "                doc.metadata[\"file_type\"]   = file_path.suffix.lower()\n",
    "                match = re.search(r\"(20\\d{2})\", file_path.name)\n",
    "                doc.metadata[\"report_year\"] = match.group(1) if match else \"unknown\"\n",
    "            docs.extend(file_docs)\n",
    "            print(f\"   \u2705 {file_path.name} \u2014 {len(file_docs)} pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"   \u26a0\ufe0f  Could not load {file_path.name}: {e}\")\n",
    "    print(f\"\\n\ud83d\udcc4 Total pages loaded: {len(docs)}\")\n",
    "    return docs\n",
    "\n",
    "# \u2500\u2500 Step 3: Build vector store \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def build_vectorstore(reports_dir=REPORTS_DIR, db_dir=CHROMA_DIR, force_rebuild=False):\n",
    "    print(f\"\\n\ud83d\udd27 Loading embedding model: {EMBEDDING_MODEL}\")\n",
    "    print(\"   (First run downloads ~90MB \u2014 takes 1-2 minutes)\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "    print(\"   \u2705 Embedding model ready\")\n",
    "\n",
    "    if os.path.exists(db_dir) and not force_rebuild:\n",
    "        print(f\"\\n\ud83d\udce6 Loading existing vector store from {db_dir}\")\n",
    "        vs    = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n",
    "        count = vs._collection.count()\n",
    "        print(f\"   \u2705 Loaded {count} chunks\")\n",
    "        return vs\n",
    "\n",
    "    docs     = load_documents(reports_dir)\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\". \",\" \",\"\"],\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"   \u2705 Created {len(chunks)} chunks\")\n",
    "\n",
    "    print(\"\\n\ud83d\udd22 Embedding and indexing...\")\n",
    "    start = time.time()\n",
    "    vs = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=db_dir,\n",
    "    )\n",
    "    # ChromaDB >= 0.4 persists automatically \u2014 no vs.persist() needed\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"   \u2705 Indexed {len(chunks)} chunks in {elapsed:.1f}s\")\n",
    "    print(f\"   \ud83d\udce6 Saved to {db_dir}\")\n",
    "    return vs\n",
    "\n",
    "print(\"\ud83d\ude80 Building RAG vector index...\")\n",
    "vectorstore = build_vectorstore()\n",
    "retriever   = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": RETRIEVAL_TOP_K, \"fetch_k\": 20},\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 RAG index ready. Proceed to Cell 6 to start the live pipeline.\")"
   ],
   "metadata": {
    "id": "cell5_rag_build"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  CELL 6 \u2014 LLM Config + Core Pipeline                        \u2551\n",
    "# \u2551  Defines prompts, ask_analyst(), and the BDH stream loop.   \u2551\n",
    "# \u2551  This is the CORE cell that connects everything together:   \u2551\n",
    "# \u2551    BDH output \u2192 LIVE_STATE \u2192 ask_analyst() \u2192 Groq LLM       \u2551\n",
    "# \u2551                                          \u2197                  \u2551\n",
    "# \u2551                              retriever (RAG / SSE docs)     \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "# \u2500\u2500 Groq client \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "# \u2500\u2500 System prompt \u2014 SSE-specific, financial-aware \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "SYSTEM_PROMPT = \"\"\"You are a senior climate, energy, and financial risk analyst\n",
    "embedded within SSE Renewables (UK & Ireland), one of Europe's leading\n",
    "renewable energy companies.\n",
    "\n",
    "COMPANY CONTEXT \u2014 SSE Renewables:\n",
    "\u2022 Listed on London Stock Exchange (SSE.L), FTSE 100 component\n",
    "\u2022 Owns and operates wind, hydro and solar assets across UK and Ireland\n",
    "\u2022 Flagship assets: Dogger Bank (world's largest offshore wind farm, 3.6GW),\n",
    "  Seagreen (1.075GW), Viking (443MW onshore), Gordonbush, Bhlaraidh\n",
    "\u2022 Net zero target: 2050 (80% reduction by 2030 vs 2018 baseline)\n",
    "\u2022 Committed \u00a318bn capital investment in low-carbon over 5 years\n",
    "\u2022 Revenue streams: Contracts for Difference (CfD), Renewable Obligation\n",
    "  Certificates (ROC), merchant power, capacity market payments\n",
    "\u2022 Turbine fleet: primarily Siemens Gamesa SG 14-236 DD (offshore),\n",
    "  Enercon E-126 / Vestas V136 (onshore)\n",
    "\u2022 Regulatory exposure: Ofgem, NESO, UK CCC climate targets\n",
    "\n",
    "FINANCIAL PARAMETERS (use these for all calculations):\n",
    "\u2022 Nominal turbine capacity    : 4.2 MW (E-126 class, onshore reference)\n",
    "\u2022 Assumed fleet size          : 50 turbines (210 MW total installed capacity)\n",
    "\u2022 CfD strike price            : \u00a398/MWh (2023 AR5 reference)\n",
    "\u2022 Merchant power price        : \u00a385/MWh (UK day-ahead average reference)\n",
    "\u2022 Annual O&M cost             : \u00a3120,000 per turbine (\u00a36M fleet total)\n",
    "\u2022 Availability factor         : 97% (industry standard onshore)\n",
    "\u2022 Transmission loss factor    : 2%\n",
    "\u2022 Carbon intensity avoided    : 0.233 tCO2e/MWh (UK grid average)\n",
    "\n",
    "BDH MODEL RULES:\n",
    "\u2022 Wind speed is extrapolated to 135m hub height via log-law\n",
    "\u2022 Power output is from physical turbine modelling \u2014 do NOT recalculate\n",
    "\u2022 memory_norm = BDH latent stability (higher = more predictable conditions)\n",
    "\u2022 All BDH numerical values are authoritative\n",
    "\n",
    "Your role IS to:\n",
    "\u2022 Write a professional monthly report with general weather/wind summary\n",
    "\u2022 Calculate and present key financial metrics using the parameters above\n",
    "\u2022 Assess physical and transition climate risks aligned with TCFD\n",
    "\u2022 Reference SSE corporate strategy and targets from the RAG documents\n",
    "\u2022 Provide actionable recommendations specific to SSE operations\n",
    "\n",
    "NEVER invent data, recalculate BDH physics, or speculate beyond provided values.\n",
    "Always show your financial calculations step by step.\n",
    "\"\"\"\n",
    "\n",
    "# \u2500\u2500 Task prompts (ALL 5 modes defined) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "TASK_PROMPTS = {\n",
    "    \"qa\": \"\"\"Answer the question clearly and concisely using the SSE report context\n",
    "and live BDH wind data provided. Cite specific sources where possible.\"\"\",\n",
    "\n",
    "    \"risk_analysis\": \"\"\"You are writing the MONTHLY OPERATIONAL REPORT for SSE Renewables.\n",
    "Structure your response EXACTLY as follows:\n",
    "\n",
    "## 1. MONTHLY WEATHER & WIND SUMMARY\n",
    "- Summarise the month's wind conditions in plain English\n",
    "- Comment on seasonal norms for UK/Ireland and how this month compares\n",
    "- Note any notable weather events (storms, calm periods, high variability)\n",
    "- Interpret BDH memory_norm: what does the stability score mean for this month\n",
    "\n",
    "## 2. ENERGY GENERATION ESTIMATE\n",
    "Using the provided avg wind speed and the fleet parameters below, calculate:\n",
    "- Estimated capacity factor (%) = (avg power output / rated capacity) \u00d7 100\n",
    "- Estimated monthly energy (MWh) = capacity factor \u00d7 total capacity (MW) \u00d7 hours\n",
    "- Show your calculation steps clearly\n",
    "- Compare to SSE's typical annual capacity factor targets (~35-40% onshore)\n",
    "\n",
    "## 3. FINANCIAL PERFORMANCE ESTIMATE\n",
    "Using CfD strike price \u00a398/MWh and fleet parameters, calculate:\n",
    "- Estimated monthly revenue (\u00a3) = MWh generated \u00d7 \u00a398\n",
    "- Estimated monthly O&M cost (\u00a3) = annual O&M / 12\n",
    "- Estimated monthly gross profit (\u00a3) = revenue \u2212 O&M\n",
    "- Lost revenue from low-wind hours = low-wind hours \u00d7 fleet capacity \u00d7 \u00a398\n",
    "- Carbon avoided (tCO2e) = MWh \u00d7 0.233\n",
    "- Show all calculations step by step\n",
    "\n",
    "## 4. PHYSICAL CLIMATE RISK ASSESSMENT (TCFD)\n",
    "- Assess risks from this month's BDH data: low-wind exposure, storm risk\n",
    "- Reference SSE's TCFD disclosures from the provided documents\n",
    "- Risk rating: HIGH / MEDIUM / LOW with specific justification from data\n",
    "\n",
    "## 5. SSE STRATEGIC ALIGNMENT\n",
    "- How does this month's performance align with SSE's net zero 2050 target?\n",
    "- Reference specific SSE commitments from the RAG documents\n",
    "- Comment on CfD exposure vs merchant price risk\n",
    "\n",
    "## 6. RECOMMENDED ACTIONS\n",
    "- 3 specific, actionable recommendations for SSE operations team\n",
    "- Prioritise by financial impact\n",
    "\n",
    "Use professional financial reporting language throughout.\n",
    "Show ALL numerical calculations explicitly.\"\"\",\n",
    "\n",
    "    \"recommendation\": \"\"\"Based on the BDH predictions and SSE report context,\n",
    "provide 3-5 actionable recommendations. Prioritise by impact and feasibility.\n",
    "Reference specific SSE strategic targets where relevant.\"\"\",\n",
    "\n",
    "    \"scenario\": \"\"\"Analyse the climate scenario implications using the BDH data\n",
    "and SSE report context. Consider 1.5\u00b0C, 2\u00b0C, and 3\u00b0C+ warming pathways.\n",
    "Focus on wind resource changes, operational risks, and portfolio resilience.\"\"\",\n",
    "\n",
    "    \"esg\": \"\"\"Provide a structured ESG/TCFD analysis using the SSE report context\n",
    "and live BDH wind data:\n",
    "1. **Governance** \u2014 oversight structures for climate risk\n",
    "2. **Strategy** \u2014 climate risk integration into business strategy\n",
    "3. **Risk Management** \u2014 identification and management processes\n",
    "4. **Metrics & Targets** \u2014 KPIs, net zero commitments, progress\"\"\",\n",
    "}\n",
    "\n",
    "# \u2500\u2500 Global Live State \u2014 updated by the BDH stream loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# This dict is the bridge between BDH outputs and the LLM\n",
    "LIVE_STATE = {\n",
    "    \"hour\":            0,\n",
    "    \"timestamp\":       None,\n",
    "    \"features\":        {},      # {feature_name: {predicted, actual, error}}\n",
    "    \"memory_norm\":     None,    # BDH latent stability proxy\n",
    "    \"wind_metrics\":    {},      # rolling wind generation summary\n",
    "    \"recent_errors\":   [],      # last N prediction errors\n",
    "}\n",
    "\n",
    "# \u2500\u2500 Core analyst function \u2014 receives BDH state + queries RAG \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def ask_analyst(question, task=\"qa\", temperature=0.3, chat_history=None,\n",
    "                bdh_data=None):\n",
    "    \"\"\"\n",
    "    The unified function that combines:\n",
    "      1. bdh_data    \u2014 direct BDH monthly output (predictions, actuals, stats)\n",
    "                       passed explicitly from stream_and_infer(); falls back to\n",
    "                       LIVE_STATE when called interactively (Cell 8 / API).\n",
    "      2. retriever   \u2014 RAG over SSE corporate documents\n",
    "    And sends both to Groq LLM for analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1\ufe0f\u20e3 Retrieve relevant SSE report chunks via RAG\n",
    "    docs          = retriever.invoke(question)\n",
    "    context_parts = []\n",
    "    sources       = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        meta = doc.metadata\n",
    "        src  = meta.get(\"source_file\", meta.get(\"source\", \"Unknown\"))\n",
    "        page = meta.get(\"page\", \"?\")\n",
    "        year = meta.get(\"report_year\", \"?\")\n",
    "        context_parts.append(\n",
    "            f\"[Excerpt {i} | {src} | Year: {year} | Page: {page}]\\n\"\n",
    "            f\"{doc.page_content.strip()}\"\n",
    "        )\n",
    "        sources.append({\"file\": src, \"year\": year, \"page\": page,\n",
    "                        \"excerpt\": doc.page_content[:200]})\n",
    "    report_context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # 2\ufe0f\u20e3 Resolve BDH data source\n",
    "    # Priority: explicit bdh_data arg (from stream loop) > LIVE_STATE (interactive)\n",
    "    if bdh_data is not None:\n",
    "        # Direct pass-in from stream_and_infer() \u2014 all fields guaranteed present\n",
    "        ms  = bdh_data[\"monthly_summary\"]    # full monthly stats dict\n",
    "        fin = bdh_data[\"financials\"]          # pre-computed financials dict\n",
    "        raw_preds = bdh_data.get(\"raw_predictions\", {})   # hourly BDH arrays\n",
    "    else:\n",
    "        # Fallback for interactive / API use after pipeline has run\n",
    "        ms  = LIVE_STATE.get(\"wind_metrics\", {})\n",
    "        fin = {}\n",
    "        raw_preds = {}\n",
    "\n",
    "    # Fleet constants (always needed)\n",
    "    TURBINE_CAPACITY_MW = 4.2\n",
    "    FLEET_SIZE          = 50\n",
    "    TOTAL_CAPACITY_MW   = TURBINE_CAPACITY_MW * FLEET_SIZE\n",
    "    CfD_PRICE           = 98.0\n",
    "    MERCHANT_PRICE      = 85.0\n",
    "    OM_ANNUAL_PER_TURB  = 120000\n",
    "    OM_MONTHLY          = (OM_ANNUAL_PER_TURB * FLEET_SIZE) / 12\n",
    "    AVAILABILITY        = 0.97\n",
    "    CARBON_FACTOR       = 0.233\n",
    "\n",
    "    # Pull values \u2014 works whether ms came from bdh_data or LIVE_STATE\n",
    "    hours      = int(ms.get(\"total_hours_processed\", ms.get(\"total_hours_processed\", 720)))\n",
    "    avg_ws     = float(ms.get(\"wind_speed_avg_ms\",   ms.get(\"avg_ws_24h\", 0)) or 0)\n",
    "    max_ws     = float(ms.get(\"wind_speed_max_ms\",   0) or 0)\n",
    "    min_ws     = float(ms.get(\"wind_speed_min_ms\",   0) or 0)\n",
    "    std_ws     = float(ms.get(\"wind_speed_std_ms\",   0) or 0)\n",
    "    wp_proxy   = float(ms.get(\"wind_power_proxy_avg\", ms.get(\"avg_power_proxy_24h\", 0)) or 0)\n",
    "    hi_wind    = int(ms.get(\"high_wind_hours_gt12ms\", ms.get(\"high_wind_hours_24h\", 0)) or 0)\n",
    "    lo_wind    = int(ms.get(\"low_wind_hours_lt4ms\",   ms.get(\"low_wind_hours_24h\", 0)) or 0)\n",
    "    calm_frac  = float(ms.get(\"calm_fraction_pct\",   0) or 0)\n",
    "    mem_avg    = ms.get(\"memory_norm_avg\",  LIVE_STATE.get(\"memory_norm\", \"N/A\"))\n",
    "    mem_std    = ms.get(\"memory_norm_std\",  \"N/A\")\n",
    "    pred_err   = ms.get(\"mean_bdh_prediction_error\", ms.get(\"mean_pred_error\", \"N/A\"))\n",
    "    month_lbl  = ms.get(\"month\", LIVE_STATE.get(\"timestamp\", \"N/A\"))\n",
    "\n",
    "    # Financials \u2014 use pre-computed if available, else re-derive\n",
    "    rated_ws        = 12.0\n",
    "    cf_approx       = min(0.45, 0.45 * (avg_ws / rated_ws) ** 3) * AVAILABILITY\n",
    "    est_energy_mwh  = fin.get(\"est_energy_mwh\",      round(cf_approx * TOTAL_CAPACITY_MW * hours, 1))\n",
    "    est_revenue_cfd = fin.get(\"est_revenue_cfd_gbp\",  round(est_energy_mwh * CfD_PRICE, 0))\n",
    "    est_gross_profit= fin.get(\"est_gross_profit_gbp\", round(est_revenue_cfd - OM_MONTHLY, 0))\n",
    "    lost_revenue    = fin.get(\"lost_revenue_gbp\",     round(lo_wind * TOTAL_CAPACITY_MW * CfD_PRICE, 0))\n",
    "    carbon_avoided  = fin.get(\"carbon_avoided_tco2e\", round(est_energy_mwh * CARBON_FACTOR, 1))\n",
    "\n",
    "    # \u2500\u2500 Raw BDH predictions block (actual vs predicted per feature) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    raw_pred_block = \"\"\n",
    "    if raw_preds:\n",
    "        # Top features most relevant to wind energy \u2014 show predicted vs actual\n",
    "        key_features = [\n",
    "            \"WS50M\", \"WS10M\", \"wind_power_50m\", \"wind_power_10m\",\n",
    "            \"T2M\", \"ALLSKY_SFC_SW_DWN\", \"RHOA\", \"RH2M\", \"PS\",\n",
    "            \"wind_shear_ratio\", \"memory_norm_last_hour\"\n",
    "        ]\n",
    "        lines = []\n",
    "        for feat in key_features:\n",
    "            if feat in raw_preds:\n",
    "                p = raw_preds[feat]\n",
    "                lines.append(\n",
    "                    f\"  {feat:<30} predicted={p['predicted_mean']:>8.4f}  \"\n",
    "                    f\"actual={p['actual_mean']:>8.4f}  \"\n",
    "                    f\"MAE={p['mae']:>8.4f}  \"\n",
    "                    f\"bias={p['bias']:>+8.4f}\"\n",
    "                )\n",
    "        if lines:\n",
    "            raw_pred_block = (\n",
    "                \"\\nBDH RAW PREDICTIONS vs ACTUALS (monthly means, key features):\\n\"\n",
    "                + \"\\n\".join(lines)\n",
    "            )\n",
    "\n",
    "    bdh_context = f\"\"\"\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "BDH PHYSICS MODEL OUTPUT \u2014 {month_lbl}\n",
    "(All values are the BDH model's own outputs \u2014 treat as ground truth)\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "MODEL PERFORMANCE:\n",
    "\u2022 Hours processed          : {hours}\n",
    "\u2022 Mean prediction error    : {pred_err}   (lower = better BDH accuracy)\n",
    "\u2022 BDH memory norm (avg)    : {mem_avg}    (higher = more stable/predictable regime)\n",
    "\u2022 BDH memory norm (std)    : {mem_std}    (lower = consistent stability)\n",
    "\n",
    "WIND RESOURCE (BDH actual outputs):\n",
    "\u2022 Average wind speed       : {avg_ws:.3f} m/s\n",
    "\u2022 Maximum wind speed       : {max_ws:.3f} m/s\n",
    "\u2022 Minimum wind speed       : {min_ws:.3f} m/s\n",
    "\u2022 Wind speed std deviation : {std_ws:.3f} m/s   (variability index)\n",
    "\u2022 Wind power proxy (avg)   : {wp_proxy:.3f}       (proportional to wind\u00b3)\n",
    "\u2022 High-wind hours >12 m/s  : {hi_wind}            (storm / high-output risk)\n",
    "\u2022 Low-wind  hours  <4 m/s  : {lo_wind}            (cut-in threshold, near-zero output)\n",
    "\u2022 Calm fraction            : {calm_frac:.1f} %     (proportion of month below cut-in)\n",
    "{raw_pred_block}\n",
    "\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "FLEET PARAMETERS & PRE-COMPUTED FINANCIALS\n",
    "(Derived from BDH wind output \u2014 use these exact values)\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "Fleet size           : {FLEET_SIZE} turbines \u00d7 {TURBINE_CAPACITY_MW} MW = {TOTAL_CAPACITY_MW:.0f} MW total\n",
    "CfD strike price     : \u00a3{CfD_PRICE}/MWh\n",
    "Merchant power price : \u00a3{MERCHANT_PRICE}/MWh\n",
    "Monthly O&M cost     : \u00a3{OM_MONTHLY:,.0f}\n",
    "Availability factor  : {AVAILABILITY*100:.0f}%\n",
    "Carbon intensity     : {CARBON_FACTOR} tCO2e/MWh\n",
    "\n",
    "FINANCIAL OUTPUTS FROM BDH WIND DATA:\n",
    "\u2022 Capacity factor (BDH)    : {cf_approx*100:.2f}%\n",
    "\u2022 Est. energy generated    : {est_energy_mwh:,.1f} MWh\n",
    "\u2022 Est. CfD revenue         : \u00a3{est_revenue_cfd:,.0f}\n",
    "\u2022 Monthly O&M cost         : \u00a3{OM_MONTHLY:,.0f}\n",
    "\u2022 Est. gross profit        : \u00a3{est_gross_profit:,.0f}\n",
    "\u2022 Lost revenue (low-wind)  : \u00a3{lost_revenue:,.0f}   ({lo_wind} hrs \u00d7 {TOTAL_CAPACITY_MW:.0f} MW \u00d7 \u00a3{CfD_PRICE})\n",
    "\u2022 Carbon avoided           : {carbon_avoided:,.1f} tCO2e\n",
    "\n",
    "INSTRUCTION: Your entire analysis MUST be grounded in the BDH numbers above.\n",
    "Do not assume or invent wind speeds, energy figures, or financial values.\n",
    "Every number in your report must trace back to BDH output or the fleet parameters.\n",
    "\"\"\"\n",
    "\n",
    "    # 3\ufe0f\u20e3 Build message list\n",
    "    task_instruction = TASK_PROMPTS.get(task, TASK_PROMPTS[\"qa\"])\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    if chat_history:\n",
    "        messages.extend(chat_history)\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "{task_instruction}\n",
    "\n",
    "{'='*60}\n",
    "SSE CORPORATE REPORT CONTEXT (RAG \u2014 retrieved documents)\n",
    "{'='*60}\n",
    "{report_context}\n",
    "\n",
    "{'='*60}\n",
    "LIVE BDH MODEL OUTPUT (continuous physics-based stream)\n",
    "{'='*60}\n",
    "{bdh_context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\"\"\"\n",
    "    })\n",
    "\n",
    "    # 4\ufe0f\u20e3 Call Groq with streaming\n",
    "    stream        = groq_client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "    full_response = \"\"\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content\n",
    "        if delta:\n",
    "            full_response += delta\n",
    "\n",
    "    return full_response, sources\n",
    "\n",
    "# \u2500\u2500 Pathway streaming simulation \u2014 monthly batches, LLM once per month \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def stream_and_infer(model, df_test, feature_names):\n",
    "    \"\"\"\n",
    "    Pathway streaming simulation:\n",
    "    - BDH inference runs EVERY HOUR (hourly predictions + LIVE_STATE update)\n",
    "    - All hourly BDH outputs are accumulated across the month\n",
    "    - At the END of each month: LLM is called ONCE with the full monthly\n",
    "      summary (BDH stats) + RAG (SSE docs) \u2192 produces one risk analysis per month\n",
    "    \"\"\"\n",
    "    all_data    = df_test.values.astype(np.float32)\n",
    "    n_total     = len(all_data)\n",
    "    global_hour = 0\n",
    "\n",
    "    # Build DatetimeIndex for monthly resampling\n",
    "    dt_index   = pd.date_range(start=test_datetimes.iloc[0],\n",
    "                               periods=len(df_test), freq=\"h\")\n",
    "    df_indexed = df_test.copy()\n",
    "    df_indexed.index = dt_index\n",
    "\n",
    "    # Feature index helpers\n",
    "    ws_idx = feature_names.index(\"WS50M\")         if \"WS50M\"         in feature_names else 0\n",
    "    wp_idx = feature_names.index(\"wind_power_50m\") if \"wind_power_50m\" in feature_names else 0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*65)\n",
    "    print(\"Pathway streaming \u2014 hourly BDH inference, monthly LLM call\")\n",
    "    print(\"=\"*65)\n",
    "\n",
    "    for month_end, month_df in df_indexed.resample(\"ME\"):\n",
    "        if month_df.empty:\n",
    "            continue\n",
    "\n",
    "        month_label     = month_end.strftime(\"%Y-%m\")\n",
    "        n_hours         = len(month_df)\n",
    "        month_start_pos = df_indexed.index.get_indexer(\n",
    "            [month_df.index[0]], method=\"nearest\"\n",
    "        )[0]\n",
    "\n",
    "        print(f\"\\n[STREAM] Month: {month_label}  ({n_hours} hourly BDH rows)\")\n",
    "\n",
    "        # \u2500\u2500 Accumulators for this month \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        month_ws_actuals   = []   # actual wind speeds each hour\n",
    "        month_ws_preds     = []   # predicted wind speeds each hour\n",
    "        month_wp_actuals   = []   # actual wind power proxy each hour\n",
    "        month_memory_norms = []   # BDH memory_norm each hour\n",
    "        month_errors       = []   # prediction errors each hour\n",
    "        last_features      = {}   # features dict from the last hour of the month\n",
    "        # Per-feature accumulation for BDH predicted vs actual arrays\n",
    "        feat_pred_sums     = {}   # sum of predicted values per feature\n",
    "        feat_act_sums      = {}   # sum of actual values per feature\n",
    "        feat_abs_err_sums  = {}   # sum of |pred - actual| per feature\n",
    "        feat_bias_sums     = {}   # sum of (pred - actual) per feature (signed)\n",
    "        feat_counts        = {}   # number of valid hours per feature\n",
    "\n",
    "        # \u2500\u2500 Hourly BDH loop (NO LLM call here) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        for local_hour in range(n_hours):\n",
    "            abs_pos = month_start_pos + local_hour\n",
    "            if abs_pos >= n_total:\n",
    "                break\n",
    "\n",
    "            # Build SEQ_LEN sliding window\n",
    "            win_start = max(0, abs_pos - SEQ_LEN + 1)\n",
    "            window    = all_data[win_start : abs_pos + 1]\n",
    "            if len(window) < SEQ_LEN:\n",
    "                pad    = np.zeros((SEQ_LEN - len(window), window.shape[1]),\n",
    "                                  dtype=np.float32)\n",
    "                window = np.vstack([pad, window])\n",
    "            window = window[-SEQ_LEN:]\n",
    "\n",
    "            # BDH inference\n",
    "            predictions, memory_norm = run_inference(model, window)\n",
    "            pred_last   = predictions[-1]\n",
    "            actual_last = all_data[abs_pos]\n",
    "\n",
    "            # Accumulate monthly stats\n",
    "            month_ws_actuals.append(float(actual_last[ws_idx]))\n",
    "            month_ws_preds.append(float(pred_last[ws_idx]))\n",
    "            month_wp_actuals.append(float(actual_last[wp_idx]))\n",
    "            month_memory_norms.append(memory_norm)\n",
    "            month_errors.append(abs(float(pred_last[ws_idx] - actual_last[ws_idx])))\n",
    "\n",
    "            # Build per-feature dict (kept from last hour for LIVE_STATE)\n",
    "            last_features = {\n",
    "                name: {\n",
    "                    \"predicted\": round(float(pred_last[i]),   4),\n",
    "                    \"actual\":    round(float(actual_last[i]), 4),\n",
    "                    \"error\":     round(float(pred_last[i] - actual_last[i]), 4),\n",
    "                }\n",
    "                for i, name in enumerate(feature_names)\n",
    "            }\n",
    "\n",
    "            # Accumulate predicted vs actual per feature across the month\n",
    "            for i, name in enumerate(feature_names):\n",
    "                p_val = float(pred_last[i])\n",
    "                a_val = float(actual_last[i])\n",
    "                feat_pred_sums[name]    = feat_pred_sums.get(name, 0.0)    + p_val\n",
    "                feat_act_sums[name]     = feat_act_sums.get(name, 0.0)     + a_val\n",
    "                feat_abs_err_sums[name] = feat_abs_err_sums.get(name, 0.0) + abs(p_val - a_val)\n",
    "                feat_bias_sums[name]    = feat_bias_sums.get(name, 0.0)    + (p_val - a_val)\n",
    "                feat_counts[name]       = feat_counts.get(name, 0)         + 1\n",
    "\n",
    "            # Update LIVE_STATE every hour so UI stays current\n",
    "            LIVE_STATE.update({\n",
    "                \"hour\":        global_hour,\n",
    "                \"timestamp\":   str(month_df.index[local_hour]),\n",
    "                \"features\":    last_features,\n",
    "                \"memory_norm\": round(memory_norm, 4),\n",
    "                \"recent_errors\": month_errors[-24:],\n",
    "                \"wind_metrics\": {\n",
    "                    \"avg_ws_24h\":          round(float(np.mean(month_ws_actuals[-24:])), 4),\n",
    "                    \"avg_power_proxy_24h\": round(float(np.mean(month_wp_actuals[-24:])), 4),\n",
    "                    \"high_wind_hours_24h\": int(sum(w > 12 for w in month_ws_actuals[-24:])),\n",
    "                    \"low_wind_hours_24h\":  int(sum(w <  4 for w in month_ws_actuals[-24:])),\n",
    "                    \"mean_pred_error\":     round(float(np.mean(month_errors)), 6),\n",
    "                },\n",
    "            })\n",
    "\n",
    "            global_hour += 1\n",
    "\n",
    "        # \u2500\u2500 END OF MONTH \u2014 build full monthly summary and call LLM ONCE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        ws_arr = np.array(month_ws_actuals)\n",
    "        wp_arr = np.array(month_wp_actuals)\n",
    "        mn_arr = np.array(month_memory_norms)\n",
    "        er_arr = np.array(month_errors)\n",
    "\n",
    "        monthly_summary = {\n",
    "            \"month\":                   month_label,\n",
    "            \"total_hours_processed\":   len(month_ws_actuals),\n",
    "            \"wind_speed_avg_ms\":       round(float(ws_arr.mean()), 3),\n",
    "            \"wind_speed_max_ms\":       round(float(ws_arr.max()),  3),\n",
    "            \"wind_speed_min_ms\":       round(float(ws_arr.min()),  3),\n",
    "            \"wind_speed_std_ms\":       round(float(ws_arr.std()),  3),\n",
    "            \"wind_power_proxy_avg\":    round(float(wp_arr.mean()), 3),\n",
    "            \"high_wind_hours_gt12ms\":  int((ws_arr > 12).sum()),\n",
    "            \"low_wind_hours_lt4ms\":    int((ws_arr <  4).sum()),\n",
    "            \"calm_fraction_pct\":       round(float((ws_arr < 4).mean() * 100), 1),\n",
    "            \"memory_norm_avg\":         round(float(mn_arr.mean()), 4),\n",
    "            \"memory_norm_std\":         round(float(mn_arr.std()),  4),\n",
    "            \"mean_bdh_prediction_error\": round(float(er_arr.mean()), 6),\n",
    "            \"last_hour_features_sample\": {\n",
    "                k: v for k, v in list(last_features.items())[:10]\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # \u2500\u2500 Build raw BDH predictions summary (monthly means per feature) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        raw_predictions = {}\n",
    "        for name in feature_names:\n",
    "            n = feat_counts.get(name, 0)\n",
    "            if n > 0:\n",
    "                raw_predictions[name] = {\n",
    "                    \"predicted_mean\": round(feat_pred_sums[name] / n, 6),\n",
    "                    \"actual_mean\":    round(feat_act_sums[name]  / n, 6),\n",
    "                    \"mae\":            round(feat_abs_err_sums[name] / n, 6),\n",
    "                    \"bias\":           round(feat_bias_sums[name]    / n, 6),\n",
    "                    \"n_hours\":        n,\n",
    "                }\n",
    "        # Also inject memory_norm as a pseudo-feature for the LLM\n",
    "        raw_predictions[\"memory_norm_last_hour\"] = {\n",
    "            \"predicted_mean\": round(float(mn_arr.mean()), 6),\n",
    "            \"actual_mean\":    round(float(mn_arr.mean()), 6),\n",
    "            \"mae\":            round(float(mn_arr.std()),  6),\n",
    "            \"bias\":           0.0,\n",
    "            \"n_hours\":        len(mn_arr),\n",
    "        }\n",
    "\n",
    "        # Update LIVE_STATE with full monthly summary before LLM call\n",
    "        LIVE_STATE[\"wind_metrics\"] = monthly_summary\n",
    "\n",
    "        # \u2500\u2500 Pretty progress bar for the month \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        calm_bar_filled = int(monthly_summary['calm_fraction_pct'] / 5)\n",
    "        calm_bar = '\u2588' * calm_bar_filled + '\u2591' * (20 - calm_bar_filled)\n",
    "\n",
    "        print(f\"\\n  \u2554{'\u2550'*63}\u2557\")\n",
    "        print(f\"  \u2551  \ud83d\udcc5  BDH MONTHLY SUMMARY \u2014 {month_label:<34}\u2551\")\n",
    "        print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "        print(f\"  \u2551  \u23f1  Hours processed     : {len(month_ws_actuals):<36}\u2551\")\n",
    "        print(f\"  \u2551  \ud83d\udca8  Avg wind speed      : {monthly_summary['wind_speed_avg_ms']:<33} m/s\u2551\")\n",
    "        print(f\"  \u2551  \ud83d\udcc8  Max wind speed      : {monthly_summary['wind_speed_max_ms']:<33} m/s\u2551\")\n",
    "        print(f\"  \u2551  \ud83d\udcc9  Min wind speed      : {monthly_summary['wind_speed_min_ms']:<33} m/s\u2551\")\n",
    "        print(f\"  \u2551  \u3030  Wind variability    : {monthly_summary['wind_speed_std_ms']:<33} m/s\u2551\")\n",
    "        print(f\"  \u2551  \u26a1  Wind power proxy    : {monthly_summary['wind_power_proxy_avg']:<36}\u2551\")\n",
    "        print(f\"  \u2551  \ud83d\udd34  High-wind hrs >12ms : {monthly_summary['high_wind_hours_gt12ms']:<36}\u2551\")\n",
    "        print(f\"  \u2551  \ud83d\udd35  Low-wind  hrs <4ms  : {monthly_summary['low_wind_hours_lt4ms']:<36}\u2551\")\n",
    "        print(f\"  \u2551  \ud83d\ude36  Calm fraction       : {monthly_summary['calm_fraction_pct']:<33} %  \u2551\")\n",
    "        print(f\"  \u2551     [{calm_bar}] {monthly_summary['calm_fraction_pct']}%{'':>10}\u2551\")\n",
    "        print(f\"  \u2551  \ud83e\udde0  BDH memory norm     : {monthly_summary['memory_norm_avg']:<36}\u2551\")\n",
    "        print(f\"  \u2551  \ud83c\udfaf  Mean predict error  : {monthly_summary['mean_bdh_prediction_error']:<36}\u2551\")\n",
    "        print(f\"  \u255a{'\u2550'*63}\u255d\")\n",
    "        print(f\"\\n  \ud83e\udd16 Sending to LLM (Groq \u00b7 {LLM_MODEL})...\\n\")\n",
    "\n",
    "        # \u2500\u2500 Single LLM call for the whole month \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        # Build the bdh_data package \u2014 everything the LLM needs, directly from BDH\n",
    "        bdh_data_for_llm = {\n",
    "            \"monthly_summary\":  monthly_summary,   # full wind stats from BDH actuals\n",
    "            \"raw_predictions\":  raw_predictions,   # per-feature predicted vs actual means\n",
    "            \"financials\": {                         # pre-computed from BDH wind output\n",
    "                \"fleet_capacity_mw\":      TOTAL_CAPACITY_MW,\n",
    "                \"capacity_factor_pct\":    round(cf_approx * 100, 2),\n",
    "                \"est_energy_mwh\":         est_energy_mwh,\n",
    "                \"est_revenue_cfd_gbp\":    est_revenue_cfd,\n",
    "                \"monthly_om_cost_gbp\":    OM_MONTHLY,\n",
    "                \"est_gross_profit_gbp\":   est_gross_profit,\n",
    "                \"lost_revenue_gbp\":       lost_revenue,\n",
    "                \"carbon_avoided_tco2e\":   carbon_avoided,\n",
    "                \"cfd_strike_price_gbp\":   CfD_PRICE,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        monthly_question = (\n",
    "            f\"Provide a full climate and operational risk analysis for {month_label}. \"\n",
    "            f\"Ground every finding in the BDH model output provided \u2014 \"\n",
    "            f\"wind speeds, predictions, memory norm, and financial figures \"\n",
    "            f\"must all come from the BDH data. \"\n",
    "            f\"Supplement with SSE corporate strategy from the RAG documents.\"\n",
    "        )\n",
    "        answer, sources = ask_analyst(\n",
    "            question    = monthly_question,\n",
    "            task        = \"risk_analysis\",\n",
    "            temperature = 0.2,\n",
    "            bdh_data    = bdh_data_for_llm,   # direct BDH output \u2192 LLM\n",
    "        )\n",
    "\n",
    "        # \u2500\u2500 Save result to JSON record \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        TURBINE_CAPACITY_MW = 4.2\n",
    "        FLEET_SIZE          = 50\n",
    "        TOTAL_CAPACITY_MW   = TURBINE_CAPACITY_MW * FLEET_SIZE\n",
    "        CfD_PRICE           = 98.0\n",
    "        OM_MONTHLY          = (120000 * FLEET_SIZE) / 12\n",
    "        AVAILABILITY        = 0.97\n",
    "        CARBON_FACTOR       = 0.233\n",
    "        hours_proc          = int(monthly_summary['total_hours_processed'])\n",
    "        avg_ws              = float(monthly_summary['wind_speed_avg_ms'])\n",
    "        rated_ws            = 12.0\n",
    "        cf_approx           = min(0.45, 0.45*(avg_ws/rated_ws)**3) * AVAILABILITY\n",
    "        est_energy_mwh      = round(cf_approx * TOTAL_CAPACITY_MW * hours_proc, 1)\n",
    "        est_revenue_cfd     = round(est_energy_mwh * CfD_PRICE, 0)\n",
    "        est_gross_profit    = round(est_revenue_cfd - OM_MONTHLY, 0)\n",
    "        low_wind_hrs        = int(monthly_summary['low_wind_hours_lt4ms'])\n",
    "        lost_revenue        = round(low_wind_hrs * TOTAL_CAPACITY_MW * CfD_PRICE, 0)\n",
    "        carbon_avoided      = round(est_energy_mwh * CARBON_FACTOR, 1)\n",
    "\n",
    "        # \u2500\u2500 Parse LLM answer into structured sections \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        def parse_llm_sections(text):\n",
    "            \"\"\"\n",
    "            Splits the LLM risk_analysis response into its 6 named sections.\n",
    "            Returns a dict with section names as keys and the text content as values.\n",
    "            \"\"\"\n",
    "            import re\n",
    "            section_map = {\n",
    "                \"weather_wind_summary\":       r\"##\\s*1\\.\",\n",
    "                \"energy_generation\":          r\"##\\s*2\\.\",\n",
    "                \"financial_performance\":      r\"##\\s*3\\.\",\n",
    "                \"climate_risk_tcfd\":          r\"##\\s*4\\.\",\n",
    "                \"strategic_alignment\":        r\"##\\s*5\\.\",\n",
    "                \"recommended_actions\":        r\"##\\s*6\\.\",\n",
    "            }\n",
    "            keys   = list(section_map.keys())\n",
    "            pats   = list(section_map.values())\n",
    "            splits = [re.search(p, text) for p in pats]\n",
    "\n",
    "            sections = {}\n",
    "            for i, key in enumerate(keys):\n",
    "                if splits[i] is None:\n",
    "                    sections[key] = \"\"\n",
    "                    continue\n",
    "                start = splits[i].start()\n",
    "                # End = start of next section, or end of string\n",
    "                end = splits[i+1].start() if i+1 < len(keys) and splits[i+1] else len(text)\n",
    "                # Strip the heading line itself, keep the body\n",
    "                body = text[start:end]\n",
    "                body = re.sub(r\"^##\\s*\\d+\\.\\s*[^\n",
    "]*\n",
    "\", \"\", body, count=1)\n",
    "                sections[key] = body.strip()\n",
    "            return sections\n",
    "\n",
    "        def extract_key_numbers(answer_text, fin):\n",
    "            \"\"\"\n",
    "            Pull the most important numbers from the LLM's own text\n",
    "            (to verify it used the BDH data correctly) plus the pre-computed\n",
    "            financials so the JSON is self-contained.\n",
    "            \"\"\"\n",
    "            import re\n",
    "\n",
    "            def find_first_number(pattern, text, default=None):\n",
    "                m = re.search(pattern, text, re.IGNORECASE)\n",
    "                if m:\n",
    "                    try:\n",
    "                        return float(m.group(1).replace(\",\", \"\"))\n",
    "                    except Exception:\n",
    "                        return default\n",
    "                return default\n",
    "\n",
    "            return {\n",
    "                # From pre-computed financials (authoritative)\n",
    "                \"capacity_factor_pct\":       round(fin[\"capacity_factor_pct\"], 2),\n",
    "                \"est_energy_mwh\":            fin[\"est_energy_mwh\"],\n",
    "                \"est_revenue_cfd_gbp\":       fin[\"est_revenue_cfd_gbp\"],\n",
    "                \"est_gross_profit_gbp\":      fin[\"est_gross_profit_gbp\"],\n",
    "                \"monthly_om_cost_gbp\":       fin[\"monthly_om_cost_gbp\"],\n",
    "                \"lost_revenue_low_wind_gbp\": fin[\"lost_revenue_gbp\"],\n",
    "                \"carbon_avoided_tco2e\":      fin[\"carbon_avoided_tco2e\"],\n",
    "                # Extracted from LLM text (what the LLM actually stated)\n",
    "                \"llm_stated_capacity_factor_pct\": find_first_number(\n",
    "                    r\"capacity factor[^\\d]*?([\\d]+\\.?\\d*)\\s*%\", answer_text),\n",
    "                \"llm_stated_energy_mwh\": find_first_number(\n",
    "                    r\"([\\d,]+)\\s*MWh\", answer_text),\n",
    "                \"llm_stated_revenue_gbp\": find_first_number(\n",
    "                    r\"\u00a3\\s*([\\d,]+).*?revenue\", answer_text),\n",
    "                \"llm_stated_risk_rating\": (\n",
    "                    re.search(r\"Risk rating[:\\s]*(HIGH|MEDIUM|LOW)\", answer_text, re.IGNORECASE)\n",
    "                    or re.search(r\"\b(HIGH|MEDIUM|LOW)\b.*?risk\", answer_text, re.IGNORECASE)\n",
    "                ),\n",
    "            }\n",
    "\n",
    "        llm_sections   = parse_llm_sections(answer)\n",
    "        fin_snapshot   = {\n",
    "            \"fleet_capacity_mw\":      TOTAL_CAPACITY_MW,\n",
    "            \"capacity_factor_pct\":    round(cf_approx * 100, 2),\n",
    "            \"est_energy_mwh\":         est_energy_mwh,\n",
    "            \"est_revenue_cfd_gbp\":    est_revenue_cfd,\n",
    "            \"monthly_om_cost_gbp\":    OM_MONTHLY,\n",
    "            \"est_gross_profit_gbp\":   est_gross_profit,\n",
    "            \"lost_revenue_gbp\":       lost_revenue,\n",
    "            \"carbon_avoided_tco2e\":   carbon_avoided,\n",
    "            \"cfd_strike_price_gbp\":   CfD_PRICE,\n",
    "        }\n",
    "        key_numbers    = extract_key_numbers(answer, fin_snapshot)\n",
    "\n",
    "        # Risk rating \u2014 pull from LLM text\n",
    "        import re as _re\n",
    "        _risk_match = (\n",
    "            _re.search(r\"Risk rating[:\\s]*(HIGH|MEDIUM|LOW)\", answer, _re.IGNORECASE) or\n",
    "            _re.search(r\"\\*\\*(HIGH|MEDIUM|LOW)\\*\\*\", answer, _re.IGNORECASE)\n",
    "        )\n",
    "        risk_rating = _risk_match.group(1).upper() if _risk_match else \"UNKNOWN\"\n",
    "\n",
    "        monthly_record = {\n",
    "            # \u2500\u2500 Identity \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            \"month\":     month_label,\n",
    "            \"timestamp\": str(LIVE_STATE.get(\"timestamp\", \"N/A\")),\n",
    "            \"bdh_hour\":  global_hour,\n",
    "\n",
    "            # \u2500\u2500 Key numbers at a glance \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            # Most important figures from both the model and the LLM \u2014 designed\n",
    "            # so you can read the JSON without opening the full analysis text.\n",
    "            \"key_numbers\": {\n",
    "                \"wind_speed_avg_ms\":         monthly_summary[\"wind_speed_avg_ms\"],\n",
    "                \"wind_speed_max_ms\":         monthly_summary[\"wind_speed_max_ms\"],\n",
    "                \"calm_fraction_pct\":         monthly_summary[\"calm_fraction_pct\"],\n",
    "                \"high_wind_hours_gt12ms\":    monthly_summary[\"high_wind_hours_gt12ms\"],\n",
    "                \"low_wind_hours_lt4ms\":      monthly_summary[\"low_wind_hours_lt4ms\"],\n",
    "                \"bdh_memory_norm_avg\":       monthly_summary[\"memory_norm_avg\"],\n",
    "                \"capacity_factor_pct\":       round(cf_approx * 100, 2),\n",
    "                \"est_energy_mwh\":            est_energy_mwh,\n",
    "                \"est_revenue_cfd_gbp\":       est_revenue_cfd,\n",
    "                \"est_gross_profit_gbp\":      est_gross_profit,\n",
    "                \"lost_revenue_low_wind_gbp\": lost_revenue,\n",
    "                \"carbon_avoided_tco2e\":      carbon_avoided,\n",
    "                \"overall_risk_rating\":       risk_rating,\n",
    "            },\n",
    "\n",
    "            # \u2500\u2500 LLM conclusion & structured analysis \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            # Each section is the LLM's own explanation, not just numbers.\n",
    "            \"llm_conclusion\": {\n",
    "                # One-sentence headline drawn from the LLM's weather summary\n",
    "                \"headline\": (\n",
    "                    llm_sections.get(\"weather_wind_summary\", \"\")\n",
    "                    .split(\"\\n\")[0].strip(\" -\u2022*\") or \"See full analysis below.\"\n",
    "                ),\n",
    "                # The LLM's overall risk verdict\n",
    "                \"overall_risk_rating\": risk_rating,\n",
    "\n",
    "                # Full narrative sections \u2014 each is the LLM's own explanation\n",
    "                \"sections\": {\n",
    "                    \"1_weather_and_wind_summary\":  llm_sections.get(\"weather_wind_summary\", \"\"),\n",
    "                    \"2_energy_generation\":         llm_sections.get(\"energy_generation\", \"\"),\n",
    "                    \"3_financial_performance\":     llm_sections.get(\"financial_performance\", \"\"),\n",
    "                    \"4_climate_risk_tcfd\":         llm_sections.get(\"climate_risk_tcfd\", \"\"),\n",
    "                    \"5_strategic_alignment\":       llm_sections.get(\"strategic_alignment\", \"\"),\n",
    "                    \"6_recommended_actions\":       llm_sections.get(\"recommended_actions\", \"\"),\n",
    "                },\n",
    "\n",
    "                # The complete unmodified LLM response (fallback / audit trail)\n",
    "                \"full_analysis_text\": answer,\n",
    "            },\n",
    "\n",
    "            # \u2500\u2500 BDH physics summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            \"bdh_summary\": {\n",
    "                \"hours_processed\":        hours_proc,\n",
    "                \"wind_speed_avg_ms\":      monthly_summary[\"wind_speed_avg_ms\"],\n",
    "                \"wind_speed_max_ms\":      monthly_summary[\"wind_speed_max_ms\"],\n",
    "                \"wind_speed_min_ms\":      monthly_summary[\"wind_speed_min_ms\"],\n",
    "                \"wind_speed_std_ms\":      monthly_summary[\"wind_speed_std_ms\"],\n",
    "                \"wind_power_proxy_avg\":   monthly_summary[\"wind_power_proxy_avg\"],\n",
    "                \"high_wind_hours_gt12ms\": monthly_summary[\"high_wind_hours_gt12ms\"],\n",
    "                \"low_wind_hours_lt4ms\":   monthly_summary[\"low_wind_hours_lt4ms\"],\n",
    "                \"calm_fraction_pct\":      monthly_summary[\"calm_fraction_pct\"],\n",
    "                \"memory_norm_avg\":        monthly_summary[\"memory_norm_avg\"],\n",
    "                \"memory_norm_std\":        monthly_summary[\"memory_norm_std\"],\n",
    "                \"mean_prediction_error\":  monthly_summary[\"mean_bdh_prediction_error\"],\n",
    "            },\n",
    "\n",
    "            # \u2500\u2500 Financial snapshot \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            \"financials\": fin_snapshot,\n",
    "\n",
    "            # \u2500\u2500 RAG sources used \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            \"sources\": [\n",
    "                {\"file\": s[\"file\"], \"year\": s[\"year\"], \"page\": s[\"page\"]}\n",
    "                for s in sources\n",
    "            ],\n",
    "        }\n",
    "        all_monthly_records.append(monthly_record)\n",
    "\n",
    "        # Save running JSON after every month (so nothing is lost mid-run)\n",
    "        with open(\"/content/sse_monthly_analysis.json\", \"w\") as jf:\n",
    "            json.dump(all_monthly_records, jf, indent=2)\n",
    "\n",
    "        # \u2500\u2500 Print formatted output \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        print(f\"  \u2554{'\u2550'*63}\u2557\")\n",
    "        print(f\"  \u2551  \ud83c\udf0d  MONTHLY RISK ANALYSIS \u2014 {month_label:<33}\u2551\")\n",
    "        print(f\"  \u2551  \u26a0\ufe0f   Mode: Risk Analysis \u00b7 Model: {LLM_MODEL:<27}\u2551\")\n",
    "        print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "        print(f\"  \u2551  \ud83d\udcb0  FINANCIAL SNAPSHOT{'':>39}\u2551\")\n",
    "        print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "        print(f\"  \u2551  Capacity factor      : {cf_approx*100:>6.1f} %{'':>34}\u2551\")\n",
    "        print(f\"  \u2551  Est. energy generated: {est_energy_mwh:>10,.0f} MWh{'':>28}\u2551\")\n",
    "        print(f\"  \u2551  Est. CfD revenue     : \u00a3{est_revenue_cfd:>11,.0f}{'':>28}\u2551\")\n",
    "        print(f\"  \u2551  Monthly O&M cost     : \u00a3{OM_MONTHLY:>11,.0f}{'':>28}\u2551\")\n",
    "        print(f\"  \u2551  Est. gross profit    : \u00a3{est_gross_profit:>11,.0f}{'':>28}\u2551\")\n",
    "        print(f\"  \u2551  Lost rev (low wind)  : \u00a3{lost_revenue:>11,.0f}{'':>28}\u2551\")\n",
    "        print(f\"  \u2551  Carbon avoided       : {carbon_avoided:>10,.1f} tCO2e{'':>26}\u2551\")\n",
    "        print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "        print(f\"  \u2551  \ud83e\udd16 LLM ANALYSIS{'':>46}\u2551\")\n",
    "        print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "        print()\n",
    "        for line in answer.split('\\n'):\n",
    "            print(f\"  {line}\")\n",
    "        print()\n",
    "        print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "        print(f\"  \u2551  \ud83d\udcce  SSE REPORT SOURCES ({len(sources)} chunks retrieved){'':>24}\u2551\")\n",
    "        print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "        if sources:\n",
    "            for i, s in enumerate(sources, 1):\n",
    "                src_line = f\"{i}. {s['file']}  (Year: {s['year']}, Page: {s['page']})\"\n",
    "                print(f\"  \u2551  {src_line:<61}\u2551\")\n",
    "        else:\n",
    "            print(f\"  \u2551  No sources retrieved \u2014 using demo documents{'':>17}\u2551\")\n",
    "        print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "        print(f\"  \u2551  \ud83d\udcbe  Saved to /content/sse_monthly_analysis.json{'':>13}\u2551\")\n",
    "        print(f\"  \u2551  \ud83d\udce1  BDH Hour: {global_hour:<8} Timestamp: {str(LIVE_STATE.get('timestamp','N/A'))[:19]:<19}  \u2551\")\n",
    "        print(f\"  \u255a{'\u2550'*63}\u255d\\n\")\n",
    "\n",
    "    # \u2500\u2500 Final save + summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    JSON_PATH = \"/content/sse_monthly_analysis.json\"\n",
    "    with open(JSON_PATH, \"w\") as jf:\n",
    "        json.dump(all_monthly_records, jf, indent=2)\n",
    "\n",
    "    file_size_kb = os.path.getsize(JSON_PATH) / 1024\n",
    "    total_energy   = sum(r['financials']['est_energy_mwh']      for r in all_monthly_records)\n",
    "    total_revenue  = sum(r['financials']['est_revenue_cfd_gbp']  for r in all_monthly_records)\n",
    "    total_profit   = sum(r['financials']['est_gross_profit_gbp'] for r in all_monthly_records)\n",
    "    total_carbon   = sum(r['financials']['carbon_avoided_tco2e'] for r in all_monthly_records)\n",
    "    total_lost_rev = sum(r['financials']['lost_revenue_gbp']     for r in all_monthly_records)\n",
    "\n",
    "    print(f\"  \u2554{'\u2550'*63}\u2557\")\n",
    "    print(f\"  \u2551  \u2705  PIPELINE COMPLETE \u2014 2-YEAR SUMMARY{'':>22}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  \ud83d\udcca  Months analysed      : {len(all_monthly_records):<35}\u2551\")\n",
    "    print(f\"  \u2551  \u23f1   Hours streamed       : {global_hour:<35,}\u2551\")\n",
    "    print(f\"  \u2551  \ud83e\udd16  LLM calls made       : {len(all_monthly_records):<35}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  \ud83d\udcb0  2-YEAR FINANCIAL TOTALS{'':>34}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  Total energy generated   : {total_energy:>12,.0f} MWh{'':>22}\u2551\")\n",
    "    print(f\"  \u2551  Total CfD revenue        : \u00a3{total_revenue:>12,.0f}{'':>22}\u2551\")\n",
    "    print(f\"  \u2551  Total gross profit       : \u00a3{total_profit:>12,.0f}{'':>22}\u2551\")\n",
    "    print(f\"  \u2551  Total lost revenue       : \u00a3{total_lost_rev:>12,.0f}{'':>22}\u2551\")\n",
    "    print(f\"  \u2551  Total carbon avoided     : {total_carbon:>12,.1f} tCO2e{'':>19}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  \ud83d\udcbe  JSON saved \u2192 {JSON_PATH}{'':>14}\u2551\")\n",
    "    print(f\"  \u2551      {len(all_monthly_records)} monthly records  |  {file_size_kb:.1f} KB{'':>24}\u2551\")\n",
    "    print(f\"  \u255a{'\u2550'*63}\u255d\\n\")\n",
    "\n",
    "    return all_monthly_records\n",
    "\n",
    "\n",
    "# \u2500\u2500 Run the full pipeline \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "import os\n",
    "\n",
    "JSON_OUTPUT_PATH = \"/content/sse_monthly_analysis.json\"\n",
    "\n",
    "print(\"\ud83d\ude80 Starting BDH stream \u2192 monthly LLM + RAG pipeline...\")\n",
    "print(f\"   BDH    : runs every hour\")\n",
    "print(f\"   LLM    : called once per month with full BDH summary + RAG\")\n",
    "print(f\"   Output : printed + saved incrementally to {JSON_OUTPUT_PATH}\")\n",
    "print(f\"   Test set: {len(df_test):,} hours (~{len(df_test)//8760:.0f} years)\")\n",
    "print()\n",
    "\n",
    "# Run pipeline \u2014 records saved incrementally inside the loop AND returned here\n",
    "all_monthly_records = stream_and_infer(bdh_model, df_test, feature_names=feature_cols)\n",
    "\n",
    "# \u2500\u2500 Confirm JSON file on disk and trigger download \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"\ud83d\udcbe JSON FILE STATUS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if os.path.exists(JSON_OUTPUT_PATH):\n",
    "    file_size_kb = os.path.getsize(JSON_OUTPUT_PATH) / 1024\n",
    "\n",
    "    # Verify it is valid JSON\n",
    "    try:\n",
    "        with open(JSON_OUTPUT_PATH) as jf:\n",
    "            saved_records = json.load(jf)\n",
    "        n = len(saved_records)\n",
    "        months = [r['month'] for r in saved_records]\n",
    "        print(f\"\u2705 File exists    : {JSON_OUTPUT_PATH}\")\n",
    "        print(f\"\u2705 File size      : {file_size_kb:.1f} KB\")\n",
    "        print(f\"\u2705 Records inside : {n} monthly analyses\")\n",
    "        print(f\"\u2705 Months covered : {months}\")\n",
    "        print(f\"\u2705 JSON is valid\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f  JSON file exists but may be corrupted: {e}\")\n",
    "\n",
    "    # \u2500\u2500 Download (Colab) or print path (local) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(f\"\\n\u2b07\ufe0f  Downloading {JSON_OUTPUT_PATH} ...\")\n",
    "        files.download(JSON_OUTPUT_PATH)\n",
    "        print(\"\u2705 Download started \u2014 check your browser downloads folder.\")\n",
    "    except ImportError:\n",
    "        print(f\"\\n\u2139\ufe0f  Running outside Colab.\")\n",
    "        print(f\"   File is at: {JSON_OUTPUT_PATH}\")\n",
    "        print(f\"   Copy it with:  !cp {JSON_OUTPUT_PATH} /your/local/path/\")\n",
    "else:\n",
    "    print(f\"\u274c JSON file NOT found at {JSON_OUTPUT_PATH}\")\n",
    "    print(\"   This means stream_and_infer() did not complete a single month.\")\n",
    "    print(\"   Check for errors above \u2014 likely causes:\")\n",
    "    print(\"   \u2022 GROQ_API_KEY missing or invalid\")\n",
    "    print(\"   \u2022 BDH inference error (feature dimension mismatch)\")\n",
    "    print(\"   \u2022 Runtime crashed mid-run (re-run Cell 6)\")\n",
    "    print()\n",
    "    # Emergency save from in-memory records if any exist\n",
    "    if all_monthly_records:\n",
    "        print(f\"   \u26a0\ufe0f  Found {len(all_monthly_records)} in-memory records \u2014 saving now...\")\n",
    "        with open(JSON_OUTPUT_PATH, \"w\") as jf:\n",
    "            json.dump(all_monthly_records, jf, indent=2)\n",
    "        print(f\"   \u2705 Emergency save complete: {JSON_OUTPUT_PATH}\")"
   ],
   "metadata": {
    "id": "cell6_stream_llm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  CELL 7 \u2014 Launch REST API (Optional)                        \u2551\n",
    "# \u2551  Run AFTER Cell 6 completes.                               \u2551\n",
    "# \u2551    POST /ask              \u2192 query the LLM analyst          \u2551\n",
    "# \u2551    GET  /monthly-reports  \u2192 download full 2-year JSON      \u2551\n",
    "# \u2551    GET  /live-state       \u2192 current BDH state              \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "# \u2500\u2500 Step 1: Install \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"-q\", \"fastapi\", \"uvicorn\",\n",
    "                \"nest_asyncio\", \"pyngrok\"], check=True)\n",
    "\n",
    "# \u2500\u2500 Step 2: Upload api.py to /content/ if not already there \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# api.py should be in the same folder as this notebook.\n",
    "# If it's not there yet, upload it via the Colab file panel (left sidebar).\n",
    "import os\n",
    "if not os.path.exists(\"/content/api.py\"):\n",
    "    print(\"\u26a0\ufe0f  api.py not found at /content/api.py\")\n",
    "    print(\"   Upload it via the Colab file panel (\ud83d\udcc1 icon) then re-run this cell.\")\n",
    "else:\n",
    "    print(\"\u2705 api.py found at /content/api.py\")\n",
    "\n",
    "# \u2500\u2500 Step 3: Launch API using notebook globals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "import sys\n",
    "sys.path.insert(0, \"/content\")\n",
    "\n",
    "from api import launch_from_notebook\n",
    "\n",
    "# Passes ask_analyst and LIVE_STATE directly from this kernel\n",
    "# so /ask uses the real trained model and RAG, not a cold start\n",
    "launch_from_notebook(\n",
    "    ask_analyst_fn  = ask_analyst,    # defined in Cell 6\n",
    "    live_state_dict = LIVE_STATE,     # defined in Cell 6\n",
    "    port            = 8000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  CELL 8 \u2014 Interactive Analyst (No widgets needed)           \u2551\n",
    "# \u2551  Works in ALL Colab environments using simple input()       \u2551\n",
    "# \u2551  Type your question when prompted, press Enter to submit    \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "QUICK_QUESTIONS = {\n",
    "    \"1\": \"What are the main physical climate risks?\",\n",
    "    \"2\": \"What is SSE's net zero target?\",\n",
    "    \"3\": \"What transition risks affect SSE?\",\n",
    "    \"4\": \"Summarise TCFD governance disclosures\",\n",
    "    \"5\": \"What ESG metrics does SSE track?\",\n",
    "    \"6\": \"What mitigation strategies are recommended?\",\n",
    "}\n",
    "\n",
    "TASK_MAP = {\n",
    "    \"1\": \"qa\",\n",
    "    \"2\": \"risk_analysis\",\n",
    "    \"3\": \"recommendation\",\n",
    "    \"4\": \"scenario\",\n",
    "    \"5\": \"esg\",\n",
    "}\n",
    "\n",
    "TASK_LABELS = {\n",
    "    \"qa\":             \"\ud83d\udcac Q&A\",\n",
    "    \"risk_analysis\":  \"\u26a0\ufe0f  Risk Analysis\",\n",
    "    \"recommendation\": \"\u2705 Recommendations\",\n",
    "    \"scenario\":       \"\ud83c\udf21\ufe0f  Scenario Analysis\",\n",
    "    \"esg\":            \"\ud83d\udcca ESG / TCFD\",\n",
    "}\n",
    "\n",
    "def print_response(question, answer, sources, task):\n",
    "    label = TASK_LABELS.get(task, task)\n",
    "    wm    = LIVE_STATE.get('wind_metrics', {})\n",
    "    print(f\"\\n  \u2554{'\u2550'*63}\u2557\")\n",
    "    print(f\"  \u2551  {label:<61}\u2551\")\n",
    "    print(f\"  \u2551  \ud83e\udd16 Model : {LLM_MODEL:<51}\u2551\")\n",
    "    print(f\"  \u2551  \ud83d\udce1 BDH   : Hour {str(LIVE_STATE.get('hour','N/A')):<7} | {str(LIVE_STATE.get('timestamp','N/A'))[:19]:<19}  \u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  \u2753 QUESTION{'':>51}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    # Wrap question at 59 chars\n",
    "    words = question.split()\n",
    "    line  = ''\n",
    "    for w in words:\n",
    "        if len(line) + len(w) + 1 > 59:\n",
    "            print(f\"  \u2551  {line:<61}\u2551\")\n",
    "            line = w\n",
    "        else:\n",
    "            line = (line + ' ' + w).strip()\n",
    "    if line:\n",
    "        print(f\"  \u2551  {line:<61}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  \ud83d\udca8 Wind Avg: {str(wm.get('wind_speed_avg_ms','N/A')):<6} m/s  \"\n",
    "          f\"Low-wind hrs: {str(wm.get('low_wind_hours_lt4ms','N/A')):<5}  \"\n",
    "          f\"High-wind hrs: {str(wm.get('high_wind_hours_gt12ms','N/A')):<4}\u2551\")\n",
    "    print(f\"  \u2551  \ud83e\udde0 BDH memory norm: {str(wm.get('memory_norm_avg','N/A')):<10}  \"\n",
    "          f\"Predict error: {str(wm.get('mean_bdh_prediction_error','N/A')):<14}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  \ud83e\udd16 ANSWER{'':>52}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print()\n",
    "    for line in answer.split('\\n'):\n",
    "        print(f\"  {line}\")\n",
    "    print()\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  \ud83d\udcce SOURCES ({len(sources)} chunks retrieved){'':>38}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    if sources:\n",
    "        for i, s in enumerate(sources, 1):\n",
    "            src_line = f\"{i}. {s['file']}  (Year: {s['year']}, Page: {s['page']})\"\n",
    "            print(f\"  \u2551  {src_line:<61}\u2551\")\n",
    "    else:\n",
    "        print(f\"  \u2551  No sources retrieved{'':>41}\u2551\")\n",
    "    print(f\"  \u255a{'\u2550'*63}\u255d\\n\")\n",
    "\n",
    "def show_menu():\n",
    "    wm = LIVE_STATE.get('wind_metrics', {})\n",
    "    print(f\"\\n  \u2554{'\u2550'*63}\u2557\")\n",
    "    print(f\"  \u2551  \ud83c\udf0d  SSE RENEWABLES \u2014 CLIMATE RISK ANALYST{'':>19}\u2551\")\n",
    "    print(f\"  \u2551  BDH Physics + RAG (SSE Docs) + Groq LLM{'':>20}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  \ud83d\udce1 BDH Hour : {str(LIVE_STATE.get('hour','N/A')):<48}\u2551\")\n",
    "    print(f\"  \u2551  \ud83d\udd50 Timestamp: {str(LIVE_STATE.get('timestamp','N/A'))[:19]:<48}\u2551\")\n",
    "    print(f\"  \u2551  \ud83d\udca8 Avg Wind : {str(wm.get('wind_speed_avg_ms','N/A')):<45} m/s\u2551\")\n",
    "    print(f\"  \u2551  \ud83e\udde0 Mem Norm : {str(wm.get('memory_norm_avg','N/A')):<48}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  ANALYSIS MODES{'':>47}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    for k, v in TASK_MAP.items():\n",
    "        print(f\"  \u2551  [{k}] {TASK_LABELS[v]:<57}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  QUICK QUESTIONS{'':>46}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    for k, v in QUICK_QUESTIONS.items():\n",
    "        print(f\"  \u2551  [q{k}] {v[:56]:<57}\u2551\")\n",
    "    print(f\"  \u2560{'\u2550'*63}\u2563\")\n",
    "    print(f\"  \u2551  [cm] Toggle chat memory ON/OFF{'':>31}\u2551\")\n",
    "    print(f\"  \u2551  [c]  Clear chat history{'':>38}\u2551\")\n",
    "    print(f\"  \u2551  [x]  Exit{'':>52}\u2551\")\n",
    "    print(f\"  \u255a{'\u2550'*63}\u255d\")\n",
    "\n",
    "# \u2500\u2500 Main interactive loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "current_task = \"qa\"\n",
    "chat_mode    = False\n",
    "\n",
    "print(\"\\n\u2705 Analyst ready. Starting interactive session...\")\n",
    "\n",
    "while True:\n",
    "    show_menu()\n",
    "\n",
    "    print(f\"  Current mode : {TASK_LABELS[current_task]}\")\n",
    "    print(f\"  Chat mode    : {'ON  (history kept)' if chat_mode else 'OFF (single query)'}\")\n",
    "    print()\n",
    "\n",
    "    user_input = input(\"  \u27a4 Enter mode number, q+number for quick question, or type your question: \").strip()\n",
    "\n",
    "    # \u2500\u2500 Exit \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    if user_input.lower() == \"x\":\n",
    "        print(\"\\n\ud83d\udc4b Exiting analyst. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # \u2500\u2500 Clear chat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    elif user_input.lower() == \"c\":\n",
    "        chat_history.clear()\n",
    "        print(\"\\n\ud83d\uddd1  Chat history cleared.\")\n",
    "        continue\n",
    "\n",
    "    # \u2500\u2500 Toggle chat mode \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    elif user_input.lower() == \"cm\":\n",
    "        chat_mode = not chat_mode\n",
    "        print(f\"\\n\ud83d\udcac Chat mode {'ON' if chat_mode else 'OFF'}\")\n",
    "        continue\n",
    "\n",
    "    # \u2500\u2500 Select analysis mode \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    elif user_input in TASK_MAP:\n",
    "        current_task = TASK_MAP[user_input]\n",
    "        print(f\"\\n\u2705 Mode set to: {TASK_LABELS[current_task]}\")\n",
    "        continue\n",
    "\n",
    "    # \u2500\u2500 Quick question \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    elif user_input.lower().startswith(\"q\") and user_input[1:] in QUICK_QUESTIONS:\n",
    "        question = QUICK_QUESTIONS[user_input[1:]]\n",
    "        print(f\"\\n  Using quick question: {question}\")\n",
    "\n",
    "    # \u2500\u2500 Custom question \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    elif len(user_input) > 3:\n",
    "        question = user_input\n",
    "\n",
    "    else:\n",
    "        print(\"\\n\u26a0\ufe0f  Not recognised. Type your question, or use the menu options above.\")\n",
    "        continue\n",
    "\n",
    "    # \u2500\u2500 Call analyst \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    print(f\"\\n  \u23f3 Analysing with mode '{TASK_LABELS[current_task]}'...\")\n",
    "    try:\n",
    "        history = chat_history if chat_mode else None\n",
    "        answer, sources = ask_analyst(\n",
    "            question,\n",
    "            task        = current_task,\n",
    "            temperature = 0.3,\n",
    "            chat_history= history,\n",
    "        )\n",
    "\n",
    "        if chat_mode:\n",
    "            chat_history.append({\"role\": \"user\",      \"content\": question})\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "            if len(chat_history) > 20:\n",
    "                chat_history[:] = chat_history[-20:]\n",
    "\n",
    "        print_response(question, answer, sources, current_task)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error: {e}\")\n",
    "        print(\"   Check your GROQ_API_KEY and that Cell 5 (RAG) ran successfully.\")"
   ],
   "metadata": {
    "id": "cell7_ui"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}